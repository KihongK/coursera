{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Step by Step\n",
    "\n",
    "이번 과제에서는 컨볼루션 신경망에서 컨볼루션과 풀링을 numpy를 통해 사용해볼겁니다. (순전파와 역전파 또한 포함됩니다!!)\n",
    "\n",
    "\n",
    "**표기법**:\n",
    "- 위첨자 $[l]$ 은 신경망의 $l^{th}$ 번째 층을 의미합니다. \n",
    "    - 예시: $a^{[4]}$ 는 $4^{th}$ 4번째 활성함수를 의미합니다. $W^{[5]}$와 $b^{[5]}$는 $5^{th}$ 번째 층의 파라미터를 의미합니다.\n",
    "\n",
    "\n",
    "- 위첨자 $(i)$ 는 $i^{th}$ 번째 데이터의 객체를 의미합니다. \n",
    "    - 예시: $x^{(i)}$ 는 입력데이터로 $i^{th}$ 번째 학습 샘플을 의미합니다.\n",
    "    \n",
    "    \n",
    "- 위첨자 $i$ 는  벡터에서 $i^{th}$ 번째 값을 의미합니다.\n",
    "    - 예시: $a^{[l]}_i$는 $l$번째 활성함수 레이어에서 $i^{th}$번째 데이터를 의미합니다\n",
    "    \n",
    "    \n",
    "- $n_H$, $n_W$ 와 $n_C$ 는 각각 높이 너비 그리고 채널 수를 의미합니다. 만약 층에 대한 정보까지 제공하고 싶다면 $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$ 로 표기합니다. \n",
    "- $n_{H_{prev}}$, $n_{W_{prev}}$ 와 $n_{C_{prev}}$ 는 각각 **이전** 층의 높이 너비 채널 수를 의미합니다. 층의 대한 정보는 이전층이니  $n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$와 같이 표기합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n",
    "\n",
    "필요한 패키지를 호출합니다.  \n",
    "- [numpy](www.numpy.org) 는 텐서 또는 행렬간의 연산 그리고 다양한 기능을 위해 사용합니다.\n",
    "- [matplotlib](http://matplotlib.org) 는 시각화를 위해 사용합니다.\n",
    "- np.random.seed(1) 는 과제 수행중 랜덤한 값을 생성하게 되는데 일정한 값이 나오도록 고정시켜 답과 같은 결과가 나오도록 하는 기능입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 개요\n",
    "\n",
    "우리는 컨볼루션 신경망의 각 블럭을 구현할겁니다. 구현해야할 각 기능에 더 세부적인 단계가 있습니다.\n",
    "\n",
    "- Convolution functions, including:\n",
    "    - Zero Padding\n",
    "    - Convolve window \n",
    "    - Convolution forward\n",
    "    - Convolution backward (optional)\n",
    "- Pooling functions, including:\n",
    "    - Pooling forward\n",
    "    - Create mask \n",
    "    - Distribute value\n",
    "    - Pooling backward (optional)\n",
    "    \n",
    "이번 과제에서는 모든 기능을 `numpy`로 처음부터 구현할겁니다.\n",
    "\n",
    "다음 과제에서는 tensorflow를 사용해 이번에 구현한 기능을 사용해 아래 사진의 모델을 만들겁니다.\n",
    "\n",
    "<img src=\"images/model.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "**Note** \n",
    "모든 순전파에 대한 역전파를 수행할 예정이기 때문에 순전파로 생긴 매개변수들을 cache에 저장해둬야합니다. 저장된 매개변수들은 역전파에 변화량을 구하는데 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Convolutional Neural Networks\n",
    "\n",
    "비록 프레임워크를 사용하면 컨볼루션을 만들기 쉽지만 딥러닝의 개념을 이해하기 어렵습니다.\n",
    "\n",
    "한번 직접 공부해 보죠 컨볼루션 레이어는 입력된 사이즈와 출력되는 사이즈가 달라집니다. 아래 그림을 보시죠~\n",
    "\n",
    "<img src=\"images/conv_nn.png\" style=\"width:350px;height:200px;\">\n",
    "\n",
    "이제 컨볼루션 층을 만들겁니다.\n",
    "\n",
    "\n",
    "첫번째로  2개 helper function을 사용할겁니다:\n",
    "1. zero padding\n",
    "2. convolution function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Zero-Padding\n",
    "\n",
    "Zero-padding은 이미지 주변 테두리에 0을 붙이는 겁니다:\n",
    "\n",
    "<img src=\"images/PAD.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 1** </u><font color='purple'>  : **Zero-Padding**<br> RGB 색상 채널을 사용하는 이미지에 padding 2  </center></caption>\n",
    "\n",
    "패딩을 적용하면 생기는 이점:\n",
    "\n",
    "- 패딩을 사용하면 CONV 층을 통해 높이와 너비가 줄어들지 않도록 합니다. 이러한 점은 깊은 신경망을 사용할때 점점 사이즈가 줄어들기 때문에 정말 중요하게 작용합니다. 또 다른 중요한 점으로 \"same-pooling\" 이라는 방법이 있는데 이 전층의 높이와 너비를 정확히 유지시켜줍니다.\n",
    "    \n",
    "- 패딩을 사용하면 이미지의 테두리 부분이 갖고있는 정보를 보존할 수 있습니다. 만약 풀링을 사용하지 않는다면 테두리 부분의 정보를 다른 픽셀들에 비해 상대적으로 조금만 사용하는겁니다. \n",
    "\n",
    "\n",
    "**Exercise**: 아래 기능은 모든 이미지의 테두리에 0값으로 둘르는 역할을 합니다. [np.pad 를 사용합니다](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html) 만약 행렬 a의 구조가 $(5,5,5,5,5)$일때 두번째 차원에 1번 두르고 네번째 차원에 3번 두르고 나머지는 그대로 유지하고 싶다면 다음 코드를 작성하면 됩니다.:  \n",
    "```python\n",
    "a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), mode='constant', constant_values = (0,0))\n",
    "    # mode 와 constatn_values 는 기본값입니다\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: zero_pad\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =\n",
      " (4, 3, 3, 2)\n",
      "x_pad.shape =\n",
      " (4, 7, 7, 2)\n",
      "x[1,1] =\n",
      " [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] =\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "\n",
    "x_pad = zero_pad(x, 2) # 따로 만든 함수로 2차원(높이) 3차원(너비) 부분에만 패딩을 2칸씩 적용한다는 의미입니다.\n",
    "print (\"x.shape =\\n\", x.shape)\n",
    "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
    "\n",
    "print (\"x[1,1] =\\n\", x[1,1])\n",
    "print (\"x_pad[1,1] =\\n\", x_pad[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```\n",
    "x.shape =\n",
    " (4, 3, 3, 2)\n",
    "x_pad.shape =\n",
    " (4, 7, 7, 2)\n",
    "x[1,1] =\n",
    " [[ 0.90085595 -0.68372786]\n",
    " [-0.12289023 -0.93576943]\n",
    " [-0.26788808  0.53035547]]\n",
    "x_pad[1,1] =\n",
    " [[ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Single step of convolution \n",
    "\n",
    "이번에는 컨볼루션은 한 단계를 수행해보겠습니다. \n",
    "입력된 이미지 중 일부분에 필터를 적용하는 것으로 컨볼루션의 아주 기본적인 역할을 하는 단위가 될겁니다.\n",
    "\n",
    "- 입력된 이미지에 대한 사이즈를 받습니다.\n",
    "- 입력된 이미지에 대한 모든 구간에 필터를 적용합니다.\n",
    "- 출력될 값의 사이즈를 받습니다 (대게 입력과 출력의 사이즈는 다릅니다)\n",
    "\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 2** </u><font color='purple'>  : **Convolution operation**<br> 필터는 3x3 사이즈이고 필터의 이동 간격을 의미하는 stride 는 1로 잡았습니다.</center></caption>\n",
    "\n",
    "필터가 입력된 이미지를 지나가면서 겹치는 각 구간은 필터와 편향값으로 계산해 위 사진 처럼 하나의 픽셀로 바뀌게 됩니다. 첫 단계로 필터와 입력이미지의 일부분(1개)으로 1개 픽셀 계산을 수행해보겠습니다. 나중에는 입력되는 값의 모든 부분에 대해 필터와의 연산을 진행할겁니다~~!!\n",
    "\n",
    "**Exercise**: conv_single_step()를 실행해보세요~. [Hint](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:   \n",
    "변수 b는 하나의 실수 또는 정수로 넘파이 행렬에 넘기면 행렬의 모든 값에 영향을 줍니다.  \n",
    "만약 넘파이 행렬에 숫자(float or integer)을 더하면 결과는 앞 넘파이 행렬 구조 그대로 나옵니다.   \n",
    "특별하게 행렬이 1개 값만 갖고있다면 정수나 실수로 변환이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: conv_single_step\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    # Element-wise product between a_slice_prev and W. Do not add the bias yet.\n",
    "    s = np.multiply(a_slice_prev, W)\n",
    "    # Sum over all entries of the volume s.\n",
    "    Z = np.sum(s)\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z = Z + float(b)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.999089450680221\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Z**\n",
    "        </td>\n",
    "        <td>\n",
    "            -6.99908945068\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - 컨볼루션 신경망의 순전파\n",
    "\n",
    "순전파에서 입력값에 대해 다양한 필터로 컨볼루션 연산을 수행합니다.\n",
    "\n",
    "각 'convolution'은 2차원 행렬을 출력으로 나오는데 우리는 출력값들을 합쳐 3차원으로 사용할겁니다:  \n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "**Exercise**: \n",
    "아래 함수를 통해 입력되는 활성함수 값 `A_prev`에 필터 `W`를 컨볼루션 연산에 사용해보세요~\n",
    "\n",
    "이 함수에 입력되는 값들은 다음과 같습니다:\n",
    "* `A_prev`는 이전 층의 활성함수로 나온 출력값입니다.\n",
    "* 필터의 구조는 `f*f`로 값들은 가중치 또는 기울기로 표현되는 `W`로 구성됩니다. .\n",
    "* 편향 벡터는 `b`입니다. 각 필터당 1개 값 b가 할당됩니다.\n",
    "\n",
    "마지막으로 패딩과 스트라이드 정보를 담고 있는 하이퍼파라미터에 접근할 수 있게됐습니다.\n",
    "\n",
    "**Hint**: \n",
    "1. \"a_prev\" (shape (5,5,3)) 에서 왼쪽 상단에 위치한 구석 부분의 2x2 를 선택하려면:\n",
    "```python\n",
    "a_slice_prev = a_prev[0:2,0:2,:]\n",
    "```\n",
    "3차원 행렬에서 높이2 너비2 채널수 3 짜리 조각을 추출하려면 아래 사진을 참고하면 좋을겁니다. 시작점과 끝점의 인덱스를 잘 생각해보세요~\n",
    "\n",
    "2. \n",
    "조각을 추출하기 위해서는 처음과 끝의 인덱스를 먼저 정의하는게 좋습니다.`vert_start`, `vert_end`, `horiz_start` 그리고 `horiz_end`.   \n",
    "아래 코드를 보면 좀더 이해하기 쉬울겁니다.\n",
    "\n",
    "\n",
    "<img src=\"images/vert_horiz_kiank.png\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 3** </u><font color='purple'>  : **자를 부분의 가로 세로의 각 시작점과 끝점을 정의합니다.** <br> 이 예시는 1개 채널용 모형입니다.  </center></caption>\n",
    "\n",
    "\n",
    "**Reminder**:\n",
    "컨볼루션 연산 이후 출력값의 구조를 계산하는 방법은 다음가 같습니다.:\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_C = \\text{number of filters used in the convolution}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: conv_forward\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, \n",
    "              numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape (≈1 line)\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. (≈1 line)\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):                               # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i]                   # Select ith training example's padded activation\n",
    "        for h in range(n_H):                         # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                     # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                 # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    \n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[...,c], b[...,c])\n",
    "                                        \n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean =\n",
      " 0.6923608807576933\n",
      "Z[3,2,1] =\n",
      " [-1.28912231  2.27650251  6.61941931  0.95527176  8.25132576  2.31329639\n",
      " 13.00689405  2.34576051]\n",
      "cache_conv[0][1][2][3] = [-1.1191154   1.9560789  -0.3264995  -1.34267579]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,5,7,4) # (높이 5, 너비 7, 채널 수 4) 10개  : 입력 데이터\n",
    "W = np.random.randn(3,3,4,8)       # (높이 3, 너비 3, 채널 수 8) : 필터 ==> 입력과 출력이 컨볼루션 연산을 거치면 8개 짜리 데이터가 생깁니다.\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 1,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\\n\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\\n\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "```\n",
    "Z's mean =\n",
    " 0.692360880758\n",
    "Z[3,2,1] =\n",
    " [ -1.28912231   2.27650251   6.61941931   0.95527176   8.25132576\n",
    "   2.31329639  13.00689405   2.34576051]\n",
    "cache_conv[0][1][2][3] = [-1.1191154   1.9560789  -0.3264995  -1.34267579]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONV 층은 나중에 활성함수 까지 거쳐야만 합니다. 미리 간단하게 보여드리자면 다음 코드 처럼 진행합니다:\n",
    "```python\n",
    "# Convolve the window to get back one output neuron\n",
    "Z[i, h, w, c] = ...\n",
    "# Apply activation\n",
    "A[i, h, w, c] = activation(Z[i, h, w, c])\n",
    "```\n",
    "\n",
    "다음에 직접 해봅시다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Pooling layer \n",
    "\n",
    "풀링 레이어는 높이와 너비를 작게만들어줍니다.\n",
    "\n",
    "풀링레이어의 이점은 연산량을 줄여줍니다. 그리고 각 위치의 특징을 잡는 역할을 합니다.\n",
    "\n",
    "풀링 레이어는 대표적으로 2개 형식을 사용합니다:\n",
    "- Max-pooling layer: 입력된 값에 대해 ($f, f$) 사이즈 윈도우로 훑으면서 각 위치에 최댓값을 저장해 출력합니다.\n",
    "\n",
    "- Average-pooling layer: 입력된 값에 대해 ($f, f$) 사이즈 윈도우로 훑으면서 각 위치 값들의 평균값을 저장해 출력합니다.\n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/max_pool1.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"images/a_pool.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "</table>\n",
    "\n",
    "풀링 레이어는 역전파에 사용할 매개변수가 없습니다. 그러나 하이퍼 파라미터로 윈도우 사이즈 $f$ 가 있습니다.\n",
    "\n",
    "\n",
    "### 4.1 - Forward Pooling\n",
    "하나의 함수에서 2개 풀링을 모두 적용해볼겁니다.\n",
    "\n",
    "**Exercise**: 아래 코드를 기억하면서 풀링레이어를 사용해봅시다.\n",
    "\n",
    "**Reminder**:\n",
    "패딩이 적용되지 않았다면 풀링이 적용된 출력값은 다음과 같습니다.\n",
    "\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "\n",
    "$$ n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: pool_forward\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A =\n",
      " [[[[1.74481176 0.90159072 1.65980218]\n",
      "   [1.74481176 1.46210794 1.65980218]\n",
      "   [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.14472371 0.90159072 2.10025514]\n",
      "   [1.14472371 0.90159072 1.65980218]\n",
      "   [1.14472371 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 0.84616065 0.82797464]\n",
      "   [0.69803203 0.84616065 1.2245077 ]\n",
      "   [0.69803203 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.84616065 1.27375593]\n",
      "   [1.96710175 0.84616065 1.23616403]\n",
      "   [1.62765075 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.86888616 1.27375593]\n",
      "   [1.96710175 0.86888616 1.23616403]\n",
      "   [1.62765075 1.12141771 0.79280687]]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A =\n",
      " [[[[-3.01046719e-02 -3.24021315e-03 -3.36298859e-01]\n",
      "   [ 1.43310483e-01  1.93146751e-01 -4.44905196e-01]\n",
      "   [ 1.28934436e-01  2.22428468e-01  1.25067597e-01]]\n",
      "\n",
      "  [[-3.81801899e-01  1.59993515e-02  1.70562706e-01]\n",
      "   [ 4.73707165e-02  2.59244658e-02  9.20338402e-02]\n",
      "   [ 3.97048605e-02  1.57189094e-01  3.45302489e-01]]\n",
      "\n",
      "  [[-3.82680519e-01  2.32579951e-01  6.25997903e-01]\n",
      "   [-2.47157416e-01 -3.48524998e-04  3.50539717e-01]\n",
      "   [-9.52551510e-02  2.68511000e-01  4.66056368e-01]]]\n",
      "\n",
      "\n",
      " [[[-1.73134159e-01  3.23771981e-01 -3.43175716e-01]\n",
      "   [ 3.80634669e-02  7.26706274e-02 -2.30268958e-01]\n",
      "   [ 2.03009393e-02  1.41414785e-01 -1.23158476e-02]]\n",
      "\n",
      "  [[ 4.44976963e-01 -2.61694592e-03 -3.10403073e-01]\n",
      "   [ 5.08114737e-01 -2.34937338e-01 -2.39611830e-01]\n",
      "   [ 1.18726772e-01  1.72552294e-01 -2.21121966e-01]]\n",
      "\n",
      "  [[ 4.29449255e-01  8.44699612e-02 -2.72909051e-01]\n",
      "   [ 6.76351685e-01 -1.20138225e-01 -2.44076712e-01]\n",
      "   [ 1.50774518e-01  2.89111751e-01  1.23238536e-03]]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 1: stride of 1\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 1, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output**\n",
    "```\n",
    "mode = max\n",
    "A.shape = (2, 3, 3, 3)\n",
    "A =\n",
    " [[[[ 1.74481176  0.90159072  1.65980218]\n",
    "   [ 1.74481176  1.46210794  1.65980218]\n",
    "   [ 1.74481176  1.6924546   1.65980218]]\n",
    "\n",
    "  [[ 1.14472371  0.90159072  2.10025514]\n",
    "   [ 1.14472371  0.90159072  1.65980218]\n",
    "   [ 1.14472371  1.6924546   1.65980218]]\n",
    "\n",
    "  [[ 1.13162939  1.51981682  2.18557541]\n",
    "   [ 1.13162939  1.51981682  2.18557541]\n",
    "   [ 1.13162939  1.6924546   2.18557541]]]\n",
    "\n",
    "\n",
    " [[[ 1.19891788  0.84616065  0.82797464]\n",
    "   [ 0.69803203  0.84616065  1.2245077 ]\n",
    "   [ 0.69803203  1.12141771  1.2245077 ]]\n",
    "\n",
    "  [[ 1.96710175  0.84616065  1.27375593]\n",
    "   [ 1.96710175  0.84616065  1.23616403]\n",
    "   [ 1.62765075  1.12141771  1.2245077 ]]\n",
    "\n",
    "  [[ 1.96710175  0.86888616  1.27375593]\n",
    "   [ 1.96710175  0.86888616  1.23616403]\n",
    "   [ 1.62765075  1.12141771  0.79280687]]]]\n",
    "\n",
    "mode = average\n",
    "A.shape = (2, 3, 3, 3)\n",
    "A =\n",
    " [[[[ -3.01046719e-02  -3.24021315e-03  -3.36298859e-01]\n",
    "   [  1.43310483e-01   1.93146751e-01  -4.44905196e-01]\n",
    "   [  1.28934436e-01   2.22428468e-01   1.25067597e-01]]\n",
    "\n",
    "  [[ -3.81801899e-01   1.59993515e-02   1.70562706e-01]\n",
    "   [  4.73707165e-02   2.59244658e-02   9.20338402e-02]\n",
    "   [  3.97048605e-02   1.57189094e-01   3.45302489e-01]]\n",
    "\n",
    "  [[ -3.82680519e-01   2.32579951e-01   6.25997903e-01]\n",
    "   [ -2.47157416e-01  -3.48524998e-04   3.50539717e-01]\n",
    "   [ -9.52551510e-02   2.68511000e-01   4.66056368e-01]]]\n",
    "\n",
    "\n",
    " [[[ -1.73134159e-01   3.23771981e-01  -3.43175716e-01]\n",
    "   [  3.80634669e-02   7.26706274e-02  -2.30268958e-01]\n",
    "   [  2.03009393e-02   1.41414785e-01  -1.23158476e-02]]\n",
    "\n",
    "  [[  4.44976963e-01  -2.61694592e-03  -3.10403073e-01]\n",
    "   [  5.08114737e-01  -2.34937338e-01  -2.39611830e-01]\n",
    "   [  1.18726772e-01   1.72552294e-01  -2.21121966e-01]]\n",
    "\n",
    "  [[  4.29449255e-01   8.44699612e-02  -2.72909051e-01]\n",
    "   [  6.76351685e-01  -1.20138225e-01  -2.44076712e-01]\n",
    "   [  1.50774518e-01   2.89111751e-01   1.23238536e-03]]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A =\n",
      " [[[[1.74481176 0.90159072 1.65980218]\n",
      "   [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 0.84616065 0.82797464]\n",
      "   [0.69803203 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.86888616 1.27375593]\n",
      "   [1.62765075 1.12141771 0.79280687]]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A =\n",
      " [[[[-0.03010467 -0.00324021 -0.33629886]\n",
      "   [ 0.12893444  0.22242847  0.1250676 ]]\n",
      "\n",
      "  [[-0.38268052  0.23257995  0.6259979 ]\n",
      "   [-0.09525515  0.268511    0.46605637]]]\n",
      "\n",
      "\n",
      " [[[-0.17313416  0.32377198 -0.34317572]\n",
      "   [ 0.02030094  0.14141479 -0.01231585]]\n",
      "\n",
      "  [[ 0.42944926  0.08446996 -0.27290905]\n",
      "   [ 0.15077452  0.28911175  0.00123239]]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 2: stride of 2\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)\n",
    "print()\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "    \n",
    "```\n",
    "mode = max\n",
    "A.shape = (2, 2, 2, 3)\n",
    "A =\n",
    " [[[[ 1.74481176  0.90159072  1.65980218]\n",
    "   [ 1.74481176  1.6924546   1.65980218]]\n",
    "\n",
    "  [[ 1.13162939  1.51981682  2.18557541]\n",
    "   [ 1.13162939  1.6924546   2.18557541]]]\n",
    "\n",
    "\n",
    " [[[ 1.19891788  0.84616065  0.82797464]\n",
    "   [ 0.69803203  1.12141771  1.2245077 ]]\n",
    "\n",
    "  [[ 1.96710175  0.86888616  1.27375593]\n",
    "   [ 1.62765075  1.12141771  0.79280687]]]]\n",
    "\n",
    "mode = average\n",
    "A.shape = (2, 2, 2, 3)\n",
    "A =\n",
    " [[[[-0.03010467 -0.00324021 -0.33629886]\n",
    "   [ 0.12893444  0.22242847  0.1250676 ]]\n",
    "\n",
    "  [[-0.38268052  0.23257995  0.6259979 ]\n",
    "   [-0.09525515  0.268511    0.46605637]]]\n",
    "\n",
    "\n",
    " [[[-0.17313416  0.32377198 -0.34317572]\n",
    "   [ 0.02030094  0.14141479 -0.01231585]]\n",
    "\n",
    "  [[ 0.42944926  0.08446996 -0.27290905]\n",
    "   [ 0.15077452  0.28911175  0.00123239]]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 전체적인 컨볼루션 신경망을 구성할수 있게 됐습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - 컨볼루션 신경망의 역전파\n",
    "\n",
    "요즘은 딥러닝 프레임워크로 인해 역전파에 대해 디테일하게 알필요가 없어졌습니다. \n",
    "컨볼루션 신경망에서 역전파를 설명하는건 매우 복잡합니다. 코스 초반 부에서 간단한 신경망으로 역전파를 구현해본 경험이 있을겁니다. 비용함수의 결과값에 대해 각 매개변수의 미분값을 구하고 매개변수를 업데이트하는 작업을 해봤습니다. 간단하게 컨볼루션 신경망에서도 비용함수 값에 대해 미분값을 구하고 매개변수를 업데이트하는 일을 합니다.\n",
    "\n",
    "영상 강의에서는 다루지 않았지만 아래 코드를 보면 간단하게 이해를 할 수 있습니다.\n",
    "\n",
    "\n",
    "### 5.1 - Convolutional layer backward pass \n",
    "\n",
    "컨볼루션 레이어에 대한 역전파!!!\n",
    "\n",
    "#### 5.1.1 - Computing dA:\n",
    "아래 수식은 $dA$를 계산하기 위한 수식입니다.  \n",
    "필터 $W_c$와 입력값의 미분을 사용해 $dA$를 계산합니다.\n",
    "\n",
    "$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "\n",
    "$W_c$는 필터이고 $dZ_{hw}$는 하나의 스칼라 값입니다.\n",
    "\n",
    "위 값들로 우리는 dA를 업데이트 하기 위해 필터 $W_c$를 매번 다른 dZ에 곱합니다.\n",
    "\n",
    "이러한 작업을 하는 이유는 우리가 순전파를 계산할때 필터를 서로 다른 조각에 대해 연산을 했기 때문입니다.\n",
    "\n",
    "그러므로 역전파로 dA를 구하기 위해 모든 조각에 대한 변화량을 구해야합니다.\n",
    "\n",
    "코드에서는 반복문이 사용될겁니다 그리고 위 식은 다음과 같이 바뀔겁니다.\n",
    "```python\n",
    "da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.2 - Computing dW:\n",
    "다음 수식은 $dW_c$ ($dW_c$는 하나의 필터에 대한 미분값입니다.) 를 구하는 방법입니다.\n",
    "\n",
    "$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "Where $a_{slice}$ corresponds to the slice which was used to generate the activation $Z_{ij}$. Hence, this ends up giving us the gradient for $W$ with respect to that slice.\n",
    "\n",
    "Since it is the same $W$, we will just add up all such gradients to get $dW$. \n",
    "\n",
    "수식을 코드로 바꾸면 다음과 같습니다.:\n",
    "```python\n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.3 - Computing db:\n",
    "\n",
    "This is the formula for computing $db$ with respect to the cost for a certain filter $W_c$:\n",
    "\n",
    "$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "\n",
    "As you have previously seen in basic neural networks, db is computed by summing $dZ$. In this case, you are just summing over all the gradients of the conv output (Z) with respect to the cost. \n",
    "\n",
    "In code, inside the appropriate for-loops, this formula translates into:\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "**Exercise**: Implement the `conv_backward` function below. You should sum over all the training examples, filters, heights, and widths. You should then compute the derivatives using formulas 1, 2 and 3 above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.45243777754\n",
      "dW_mean = 1.72699145831\n",
      "db_mean = 7.83923256462\n"
     ]
    }
   ],
   "source": [
    "# We'll run conv_forward to initialize the 'Z' and 'cache_conv\",\n",
    "# which we'll use to test the conv_backward function\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "# Test conv_backward\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output: **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **dA_mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            1.45243777754\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **dW_mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            1.72699145831\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **db_mean**\n",
    "        </td>\n",
    "        <td>\n",
    "            7.83923256462\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Pooling layer - backward pass\n",
    "\n",
    "다음으로 풀링레이어에 대한 역전파를 수행해보겠습니다. 일단 맥스 풀링을 먼저 실행해보죠~\n",
    "\n",
    "풀링 레이어는 역전파로 업데이트할 매개변수가 없더라도 이전층의 그레디언트를 계산하기 위해 필요합니다!!\n",
    "\n",
    "### 5.2.1 Max pooling - backward pass  \n",
    "\n",
    "풀링레이어에 역전파로 바로 들어가기전에 다음 함수 `create_mask_from_window()`를 선언을 해야합니다.   \n",
    "$$ X = \\begin{bmatrix}\n",
    "1 && 3 \\\\\n",
    "4 && 2\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix}\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보시다시피 이 함수는 \"mask\" 기능을하는 행렬을 만드는 겁니다. 마스크는 행렬에서 최댓값이 어디에 위치했는지 알려주는 역할을 하죠\n",
    "\n",
    "True 에 해당하는 값 1을 X 행렬에서 최대값이 있는 위치를 의미하고 나머지 위치는 False (0) 값으로 채워집니다. 나중에 average 풀링 레이어에서 다른 방법의 \"mask\"를 사용할겁니다. 기대하세요~\n",
    "\n",
    "**Exercise**: 풀링 레이어에 역전파를 위한 함수 `create_mask_from_window()`를 생성하세요\n",
    "\n",
    "Hints:\n",
    "- [np.max()]() 를 사용하면 행렬의 최댓값을 알수 있습니다.\n",
    "- 행렬 X와 하나의 값인 스칼라 x가 있다면 `A = (X == x)`를 사용해 새로운 행렬 A를 만듭니다. (구조는 X와 같은):\n",
    "```\n",
    "A[i,j] = True if X[i,j] = x\n",
    "A[i,j] = False if X[i,j] != x\n",
    "```\n",
    "- 이부분에서는 최댓값이 여러개인 상황에 대해 신경쓰지 않습니다!=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    mask = x == np.max(x)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:** \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**x =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "[[ 1.62434536 -0.61175641 -0.52817175] <br>\n",
    " [-1.07296862  0.86540763 -2.3015387 ]]\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**mask =**\n",
    "</td>\n",
    "<td>\n",
    "[[ True False False] <br>\n",
    " [False False False]]\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 최댓값의 위치를 알아야할까요?  \n",
    "왜냐하면 최댓값이 출력값에 영향을 미쳤고 결과적으로 비용함수에 영향을 미쳤기 때문에 고려해야하기 때문입니다. 역전파는 비용함수 값에 대한 그레디언트를 계산하는 것으로 비용함수에 영향을 끼친 무엇이든 그레디언트가 0이 아니게 됩니다. 그래서 비용함수에 영향을 끼친 모든 것에 그레디언트 값이 존재하게 되고 이렇게 특정한 위치에 있는 값에 대해 그레디언트를 계산해야합니다.  \n",
    "하지만 최댓값이 아닌 값들은 비용함수에 영향을 안줬기 때문에 모두 0으로 처리하는 겁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 - Average pooling - backward pass \n",
    "\n",
    "맥스 풀링은 각 윈도우에서 최댓값 1개만 비용함수에 영향을 미칩니다.\n",
    "\n",
    "평균 풀링은 윈도우에 해당하는 모든 요소가 균등하게 영향을 미칩니다.\n",
    "\n",
    "그래서 역전파를 하기 위해 다른 helper function 을 사용해야합니다.\n",
    "\n",
    "예를 들어 2x2 필터를 사용한 평균 풀링을 사용했다면 역전파를 위한 마스크 행렬은 다음과 같을 겁니다.:\n",
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "1/4 && 1/4 \\\\\n",
    "1/4 && 1/4\n",
    "\\end{bmatrix}\\tag{5}$$\n",
    "\n",
    "평균을 사용하므로 각 위치의 값들이 비용함수에 균등한 영향을 끼쳤기 때문입니다.\n",
    "\n",
    "**Exercise**:   \n",
    "dz 값을 구조에 맞으면서 균등하게 분배하는 함수를 구현합니다. [Hint](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ones.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones(shape) * average\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "distributed_value =\n",
    "</td>\n",
    "<td>\n",
    "[[ 0.5  0.5]\n",
    "<br\\> \n",
    "[ 0.5  0.5]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 함수를 하나로 합치기 (Pooling layer backprop)\n",
    "\n",
    "**Exercise**: \n",
    "`pool_backward` 함수로 두가지 풀링 옵션을 다룰 수 있도록 만들어봅시다. 이전과 같이 한번 더 4개 for 문을 사용해야합니다. (데이터 샘플, 높이, 너비, 채널 수) 또 `if/elif`문을 사용해 조건에 맞으면 `max` 또는 `average`를 사용하도록 만듭니다. 만약 'average'가 입력된다면 위에서 만든 `distribute_value()`함수를 사용해야합니다. 반대로 'max'가 입력되면 `create_mask_from_window()`함수를 사용해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "mode = max:\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**mean of dA =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "0.145713902729\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**dA_prev[1,1] =** \n",
    "</td>\n",
    "<td>\n",
    "[[ 0.          0.        ] <br>\n",
    " [ 5.05844394 -1.68282702] <br>\n",
    " [ 0.          0.        ]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "mode = average\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**mean of dA =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "0.145713902729\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**dA_prev[1,1] =** \n",
    "</td>\n",
    "<td>\n",
    "[[ 0.08485462  0.2787552 ] <br>\n",
    " [ 1.26461098 -0.25749373] <br>\n",
    " [ 1.17975636 -0.53624893]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "qO8ng",
   "launcher_item_id": "7XDi8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
