{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition\n",
    "\n",
    "이번에는 얼굴인식 기능을 구현해보겠습니다.\n",
    "\n",
    "오늘 배울 내용의 아이디어는 [FaceNet](https://arxiv.org/pdf/1503.03832.pdf)에서 참고했습니다. 이번 강의에서 [DeepFace](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)에 대한 내용도 다룹니다.\n",
    "\n",
    "얼굴인식의 대표적인 문제는 2가지로 설명합니다.\n",
    "\n",
    "- **Face Verification** - \n",
    "예를들어 공항에서 세관을 통과하기 위해서는 여권을 스캔하고 여권의 얼굴과 실제 얼굴을 대조해서 맞아야 통과할수 있습니다.\n",
    "또 다른 예로 휴대폰 얼굴인식 잠금해제를 사용할때도 face verification 이 필요합니다.\n",
    "이 문제는 1:1 로 매칭이 되는지를 묻는 문제입니다.\n",
    "\n",
    "- **Face Recognition** - \n",
    "예를 들어 [face recognition video](https://www.youtube.com/watch?v=wr4rx0Spihs)를 보시면 직원이 사무실로 갈때 따로 신분증이 필요없습니다. 단지 얼굴 인식으로만 신분을 확인하는 겁니다. \n",
    "이 문제는 1:K 로 매칭이 되는지를 묻는 문제입니다.\n",
    "\n",
    "FaceNet은 얼굴 이미지를 128개 숫자 벡터로 변환하는 신경망 구조입니다. 두 사진으로 부터 나온 두 벡터를 비교해 두 사진이 같은 사람인지 결정하게됩니다.\n",
    "\n",
    "**In this assignment, you will:**\n",
    "- triplet 손실함수를 구현합니다.\n",
    "- 학습된 모델을 사용해 얼굴 사진을 128차원 벡터로 인코딩합니다.\n",
    "- 인코딩을 통해 face verification과 recognition을 이해합니다.\n",
    "\n",
    "#### Channels-first notation\n",
    "\n",
    "* 이번 노트북에서는 **\"channels first\"**를 사용합니다. 대부분 그리고 예전 강의에서는 **\"channels last\"**를 사용합니다. 단지 자료 구조에서 channel 부분이 앞으로 온다는 의미입니다. \n",
    "* default값은 channels last는 (M, Height, Width, Channel)으로 구성되고 channel first는 (M, Channel, Height, Width)로 구성됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Naive Face Verification\n",
    "\n",
    "Face Verification은 두 이미지를 받아 두 이미지가 같은 인물인지 아닌지를 결정합니다.\n",
    "\n",
    "가장 기초적인 방법으로는 두 이미지의 픽셀값들의 거리를 비교하는 방법입니다.\n",
    "\n",
    "두 이미지의 거리감이 임계치보다 낮을때 그때 같은 사람이라고 설명할 수 있게됩니다.\n",
    "\n",
    "<img src=\"images/pixel_comparison.png\" style=\"width:380px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 1** </u></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 픽셀값이 햇빛, 각도, 얼굴 위치 다양한 요소에 의해 급격하게 변하기 때문에 이 방법은 정말 성능이 좋지 않을겁니다. \n",
    "* 단순히 픽셀값을 비교하는게 아닌 인코딩으로 나온 이미지$f(img)$를 비교하면 좀 더 정확한 판단을 할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Encoding face images into a 128-dimensional vector \n",
    "\n",
    "### 1.1 - Using a ConvNet  to compute encodings\n",
    "\n",
    "FaceNet 모델은 많은 데이터를 사용하기 때문에 학습에 오랜시간이 걸립니다. 그러한 이유로 미리학습된 모델을 사용해 실습을 진행하겠습니다.\n",
    "신경망 구조는 [Szegedy *et al.*](https://arxiv.org/abs/1409.4842)에서 말한 Inception 모델 구조를 사용합니다.\n",
    "실습을 위해 Inception 구조 또한 모듈로 제공합니다.\n",
    "\n",
    "모듈에 자세한 내용은 `inception_blocks_v2.py`파일이 있으니 확인해보세요~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알아야하는 핵심 정보:\n",
    "\n",
    "- 이 신경망은 입력값으로 96x96 사이즈 3채널 이미지를 받습니다. 구체적으로 얼굴 이미지를 사용하도록 하고 데이터 전체 구조는 $(m, n_C, n_H, n_W) = (m, 3, 96, 96)$ 입니다.\n",
    "- 출력값은 128차원 벡터로 전체 구조는 $(m, 128)$입니다.\n",
    "\n",
    "아래 셀을 실행시켜 모델 객체를 만들어보세요~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3743280\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Params:\", FRmodel.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output **\n",
    "<table>\n",
    "<center>\n",
    "Total Params: 3743280\n",
    "</center>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 인코딩한 결과로 2개 이미지를 비교해봅시다\n",
    "\n",
    "\n",
    "<img src=\"images/distance_kiank.png\" style=\"width:680px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 2**: <br> </u> <font color='purple'> 설정한 임계치(거리)와 두 이미지의 거리를 비교해 같은 사람인지 아닌지를 결정합니다.\n",
    "</center></caption>\n",
    "\n",
    "So, an encoding is a good one if: \n",
    "- 같은 사람이미지 두장을 인코딩 했을때 두 결과값은 비슷합니다. (두 벡터의 거리는 낮게 나옵니다)\n",
    "- 다른 사람이미지를 인코딩 했을때 두 벡터의 거리는 매우 크게 나옵니다.\n",
    "\n",
    "triplet 손실함수는 3개 이미지를 사용합니다.  \n",
    "    3개 이미지는 비교대상 이미지 1장과 같은 사람이미지 1장 그리고 다른 사람이미지 1장을 사용하는데 이 손실함수는 비교대상이미지와 같은 사람이미지 거리는 더 가깝게 다른 사람이미지는 더 멀게 되도록 학습합니다.\n",
    "\n",
    "<img src=\"images/triplet_comparison.png\" style=\"width:280px;height:150px;\">\n",
    "<br>\n",
    "<caption><center> <u> <font color='purple'> **Figure 3**: <br> </u> <font color='purple'> \n",
    "    다음부터 이미지는 왼쪽부터 이런식으로 표현하겠습니다 (Anchor(A), Positive(P), Negative(N))</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1.2 - The Triplet Loss\n",
    "\n",
    "이미지 $x$를 인코딩하면 $f(x)$로 표현합니다. $f$는 신경망으로 계산한다는 의미입니다.\n",
    "\n",
    "<img src=\"images/f_x.png\" style=\"width:380px;height:150px;\">\n",
    "\n",
    "<!--\n",
    "모델 마지막 부분에 정규화 과정도 추가합니다.\n",
    "We will also add a normalization step at the end of our model so that $\\mid \\mid f(x) \\mid \\mid_2 = 1$ (means the vector of encoding should be of norm 1).\n",
    "!-->\n",
    "\n",
    "이미지 $(A, P, N)$와 triplet 함수로 학습합니다.\n",
    "- A는 \"Anchor\" 이미지로 비교대상 사람 이미지입니다\n",
    "- P는 \"Positive\" 이미지로 Anchor 이미지와 같은 사람이미지입니다.\n",
    "- N는 \"Negative\" 이미지로 Anchor 이미지와 다른 사람이미지입니다.\n",
    "\n",
    "이러한 triplets는 학습데이터셋에서 선택됩니다. 그리고 우리는 $(A^{(i)}, P^{(i)}, N^{(i)})$로 표기하고 $i$번째 학습 데이터라고 표현하겠습니다. \n",
    "\n",
    "확실하게 하기 위해 이미지 $A^{(i)}$가 Positive 이미지 $P^{(i)}$에게 최소 $\\alpha$ 만큼 더 가까워지게 만들고 싶을겁니다. (Negative image 이미지 $N^{(i)}$보다)\n",
    "\n",
    "$$\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 + \\alpha < \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$$\n",
    "\n",
    "그리고 \"triplet cost\"를 최소화 시키고싶을겁니다.:\n",
    "\n",
    "$$\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\underbrace{\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha \\large ] \\small_+ \\tag{3}$$\n",
    "\n",
    "여기서 \"$[z]_+$\" 표기법을 사용하는데 $max(z,0)$로 이해하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- (1) 수식은 기준인 anchor 이미지 \"A\"와 positive 이미지 \"P\"의 거리를 제곱한 값입니다. 이 값을 최소화 시키는게 목표입니다.\n",
    "- (2) 수식은 기준인 anchor 이미지 \"A\"와 negative 이미지 \"N\"의 거기를 제곱한 값입니다. 이 값은 상대적으로 더 크게 만드는 것이 목표입니다. 이 수식에는 -(마이너스)가 붙는데요 손실함수는 최소화 하는게 목표인데 이 수식은 크게 만드는 것이 목표라 -를 추가해줍니다.\n",
    "- $\\alpha$ 는 마진으로 이해합니다. 하이퍼파라미터로 우리는 $\\alpha = 0.2$를 사용합니다.\n",
    "\n",
    "대부분 인코딩으로 나온 벡터를 rescale 작업으로 L2 norm 값을 1로 만듭니다. ($\\mid \\mid f(img)\\mid \\mid_2$=1) 크게 걱정하실 필요는 없습니다.\n",
    "\n",
    "\n",
    "**Exercise**: 위에서 정의한 triplet 손실함수를 구현하기 위해서는 다음 4단계를 거쳐야합니다.\n",
    "1. \"anchor\"와 \"positive\"의 거리를 계산합니다. : $\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2$\n",
    "2. \"anchor\"와 \"negative\"의 거리를 계산합니다. : $\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$\n",
    "3. 1,2 수식을 사용해 1개 데이터에 대한 triplet cost 계산식을 만듭니다:$ \\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 - \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2 + \\alpha$\n",
    "4. 전체 학습데이터를 사용해 비용함수를 계산하는 식을 만듭니다.$$\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 - \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2+ \\alpha \\large ] \\small_+ \\tag{3}$$\n",
    "\n",
    "#### Hints\n",
    "* 유용한 기능: `tf.reduce_sum()`, `tf.square()`, `tf.subtract()`, `tf.add()`, `tf.maximum()`.\n",
    "\n",
    "#### Additional Hints\n",
    "* L2norm 에 제곱한 것은 차이 제곱의 합과 같습니다. $||x - y||_{2}^{2} = \\sum_{i=1}^{N}(x_{i} - y_{i})^{2}$\n",
    "\n",
    "* `anchor`, `positive`와 `negative`의 구조는 `(m,128)`입니다. m은 데이터셋 수를 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: triplet_loss\n",
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(y_pred[0],y_pred[1])), axis=-1)\n",
    "    \n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(y_pred[0],y_pred[2])), axis=-1)\n",
    "    \n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist,neg_dist),alpha)\n",
    "    \n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss,0.0))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 528.143\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as test:\n",
    "    tf.set_random_seed(1)\n",
    "    y_true = (None, None, None)\n",
    "    y_pred = (tf.random_normal([3, 128], mean=6, stddev=0.1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=1, stddev=1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=3, stddev=4, seed = 1))\n",
    "    loss = triplet_loss(y_true, y_pred)\n",
    "    \n",
    "    print(\"loss = \" + str(loss.eval()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **loss**\n",
    "        </td>\n",
    "        <td>\n",
    "           528.143\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Loading the pre-trained model\n",
    "\n",
    "FaceNet은 triplet loss를 최소화 하도록 학습됐습니다.\n",
    "\n",
    "그러나 모델을 학습시키는 데에는 엄청난 양의 데이터와 연산시간이 필요합니다. 그래서 우리는 학습을 처음부터 진행하지 않을겁니다.\n",
    "\n",
    "대신에 미리 학습된 모델을 사용해 시간을 절약할겁니다. 아래 코드를 실행시켜 학습된 모델을 호출하세요. (약간 시간이 걸릴 수 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 사진을 보시면 3명의 이미지를 통해 거리를 구한 예시가 있습니다. (1인당 2장)\n",
    "\n",
    "<img src=\"images/distance_matrix.png\" style=\"width:380px;height:200px;\">\n",
    "<br>\n",
    "<caption><center> <u> <font color='purple'> **Figure 4**:</u> <br>  <font color='purple'> 3명의 이미지에 각 거리</center></caption>\n",
    "\n",
    "얼굴 인식과 분별을 하기위해 모델을 사용해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Applying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "건물에 직원의 얼굴을 인식해 출입을 허용하는 시스템을 개발한다고 생각해봅시다.  \n",
    "일단 당신은 해당 건물에서 근무하는 사람들 정보를 제공받는 **Face verification**을 만들어야합니다. 과정을 생각해보면 근무자들은 출입을 위해 사람들은 ID카드를 사용하고 그때 face recognition system은 카드에 등록된 이미지와 대조합니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Face Verification\n",
    "\n",
    "그럼 일단 데이터 베이스를 구축합시다. 데이터 베이스에는 근무자 이미지를 벡터로 변환한 값을 저장합니다.\n",
    "\n",
    "이미지를 벡터로 변환하는 기능 `img_to_encoding(image_path, model)`을 사용해 벡터를 생성합니다.\n",
    "\n",
    "아래 코드를 사용해 데이터베이스를 만들고 데이터를 저장하세요~\n",
    "\n",
    "데이터 베이스는 근무자의 이름과 128차원 벡터가 매핑되어 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"danielle\"] = img_to_encoding(\"images/danielle.png\", FRmodel)\n",
    "database[\"younes\"] = img_to_encoding(\"images/younes.jpg\", FRmodel)\n",
    "database[\"tian\"] = img_to_encoding(\"images/tian.jpg\", FRmodel)\n",
    "database[\"andrew\"] = img_to_encoding(\"images/andrew.jpg\", FRmodel)\n",
    "database[\"kian\"] = img_to_encoding(\"images/kian.jpg\", FRmodel)\n",
    "database[\"dan\"] = img_to_encoding(\"images/dan.jpg\", FRmodel)\n",
    "database[\"sebastiano\"] = img_to_encoding(\"images/sebastiano.jpg\", FRmodel)\n",
    "database[\"bertrand\"] = img_to_encoding(\"images/bertrand.jpg\", FRmodel)\n",
    "database[\"kevin\"] = img_to_encoding(\"images/kevin.jpg\", FRmodel)\n",
    "database[\"felix\"] = img_to_encoding(\"images/felix.jpg\", FRmodel)\n",
    "database[\"benoit\"] = img_to_encoding(\"images/benoit.jpg\", FRmodel)\n",
    "database[\"arnaud\"] = img_to_encoding(\"images/arnaud.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 누군가가 정문에서 ID카드를 사용한다면(이름을 제공) 데이터 베이스에서 같은 이름을 찾고 해당 정보와 매핑되는 벡터를 사용해 정문앞에 서있는 사람과 맞는지 확인하는 과정이 진행됩니다.\n",
    "\n",
    "**Exercise**: verify() 기능을 실행시켜 근무자를 확인지 알아보세요~. \n",
    "\n",
    "다음 단계를 차근차근 따라가보세요:\n",
    "1. 새롭게 입력되는 이미지(출입문에 카메라)를 벡터화 시키세요~\n",
    "2. 새롭게 입력되는 이미지의 벡터와 데이터 베이스에서 ID 카드가 제공하는 이름과 매핑되는 벡터의 거리를 계산하세요\n",
    "3. 거리 임계치보다 작게 나오면 문을 열어주고 아니면 열리지 않는 조건문을 만드세요~\n",
    "\n",
    "\n",
    "* L2 distance 는 [np.linalg.norm](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html)로 사용하세요~\n",
    "\n",
    "#### Hints\n",
    "* `identity`는 문자열 데이터이고 `database`사전에 key값입니다.\n",
    "* `img_to_encoding`는 2개 변수를 입력받습니다: `image_path`와 `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: verify\n",
    "def verify(image_path, identity, database, model):\n",
    "    \"\"\"\n",
    "    Function that verifies if the person on the \"image_path\" image is \"identity\".\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    identity -- string, name of the person you'd like to verify the identity. Has to be an employee who works in the office.\n",
    "    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    dist -- distance between the image_path and the image of \"identity\" in the database.\n",
    "    door_open -- True, if the door should open. False otherwise.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line)\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    \n",
    "    # Step 2: Compute distance with identity's image (≈ 1 line)\n",
    "    dist = np.linalg.norm(database[identity] - encoding)\n",
    "    \n",
    "    # Step 3: Open the door if dist < 0.7, else don't open (≈ 3 lines)\n",
    "    if dist < 0.7:\n",
    "        print(\"It's \" + str(identity) + \", welcome in!\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(\"It's not \" + str(identity) + \", please go away\")\n",
    "        door_open = False\n",
    "        \n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Younes 는 사무실에 출입하려고 했고 출입문 카메라가 그의 사진을 찍었습니다.\n",
    "\n",
    "찍은 사진으로 verification 알고리즘에 적용시켜봅시다.\n",
    "\n",
    "<img src=\"images/camera_0.jpg\" style=\"width:100px;height:100px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's younes, welcome in!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.65939289, True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_0.jpg\", \"younes\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **It's younes, welcome in!**\n",
    "        </td>\n",
    "        <td>\n",
    "           (0.65939283, True)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 건물에서 근무하지 Benoit이 Kian의 ID카드를 훔쳐 출입을 시도한다고 해봅시다.\n",
    "\n",
    "출입문 카메라가 Benoit사진을 찍었습니다.\n",
    "찍은 사진으로 verification 알고리즘에 적용시켜봅시다.\n",
    "\n",
    "<img src=\"images/camera_2.jpg\" style=\"width:100px;height:100px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's not kian, please go away\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.86224014, False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_2.jpg\", \"kian\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **It's not kian, please go away**\n",
    "        </td>\n",
    "        <td>\n",
    "           (0.86224014, False)\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Face Recognition\n",
    "\n",
    "face verification 시스템은 어느정도 잘 작동합니다.\n",
    "\n",
    "그러나 Kian은 여전히 ID카드를 도난당한 상태입니다. Kian은 다음날 카드가 없어 들어갈 수 없을겁니다.\n",
    "\n",
    "이러한 문제를 해결하기 위해 face verification 시스템을 face recognition으로 바꿀필요가 있습니다.\n",
    "\n",
    "이 방법을 사용하면 더 이상 ID 카드가 필요없어집니다.\n",
    "\n",
    "인증받은 사람은 그냥 아무것도 없이 출입이 가능해집니다.\n",
    "\n",
    "face recognition를 만든다면 사진을 찍고 해당 사진이 인증 받은 사람인지 확인 후 맞다면 누구인지 알려주게 됩니다.\n",
    "\n",
    "앞에서와 다르게 우리는 사람의 이름을 입력값으로 더이상 사용하지 않습니다.\n",
    "\n",
    "\n",
    "**Exercise**: `who_is_it()`를 구현하기 위해 다음 단계를 밟아야합니다.\n",
    "1. 입력되는 이미지(출입문에서 찍은 사진)을 벡터화 합니다.\n",
    "2. 데이터 베이스에서 입력이미지와 가장 가까운 값을 갖는 벡터를 찾습니다.\n",
    "    - `min_dist`를 큰 값으로 초기화 합니다. 이 값은 이미지가 입력될때 마다 초기화 되고 데이터 베이스조회 할때마다 바뀔수 있습니다.\n",
    "    \n",
    "    - 모든 데이터베이스를 조회합니다. 데이터 베이스를 조회할때는 `for (name, db_enc) in database.items()`를 사용합니다.\n",
    "        - 입력 이미지의 벡터와 데이터 베이스에서 얻은 벡터의 L2 거리를 계산합니다.\n",
    "        - 거리가 min_dist보다 작다면 해당 거리를 min_dist로 업데이트하고 identitiy를 name으로 업데이트 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: who_is_it\n",
    "\n",
    "def who_is_it(image_path, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the office by finding who is the person on the image_path image.\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    database -- database containing image encodings along with the name of the person on the image\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "    identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)\n",
    "    encoding = img_to_encoding( image_path, model )\n",
    "    \n",
    "    ## Step 2: Find the closest encoding ##\n",
    "    \n",
    "    # Initialize \"min_dist\" to a large value, say 100 (≈1 line)\n",
    "    min_dist = 100\n",
    "    \n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        # Compute L2 distance between the target \"encoding\" and the current db_enc from the database. (≈ 1 line)\n",
    "        dist = np.linalg.norm(db_enc - encoding)\n",
    "\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "    \n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not in the database.\")\n",
    "    else:\n",
    "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Younes 는 사무실에 출입하려고 했고 출입문 카메라가 그의 사진을 찍었습니다.\n",
    "\n",
    "찍은 사진으로 who_is_it 알고리즘에 적용시켜봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's younes, the distance is 0.659393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.65939289, 'younes')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "who_is_it(\"images/camera_0.jpg\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **it's younes, the distance is 0.659393**\n",
    "        </td>\n",
    "        <td>\n",
    "           (0.65939283, 'younes')\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congratulations!\n",
    "\n",
    "* face recognition 시스템이 잘 작동합니다. 이 시스템은 오로지 인증된 사람들만 입장시켜줍니다.\n",
    "* 더이상 ID카드는 필요없어졌습니다.\n",
    "\n",
    "#### Ways to improve your facial recognition model\n",
    "좀 더 알고리즘을 개선하는 방법은 다음과 같습니다.\n",
    "\n",
    "- 각 사람마다 더 많은 이미지(서로 다른 조건 ex 밝기, 날씨, 헤어스타일,...)를 데이터베이스에 넣습니다.\n",
    "- 최대한 얼굴 정보만 담을 수 있도록 이미지를 crop 합니다. 이러한 과정은 얼굴과 무관한 픽셀을 줄여 모델이 안정적인 성능을 보이게 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심!!\n",
    "- Face verification은 1:1 매칭하는 문제입니다. face recognition은 1:K 매칭 문제입니다.\n",
    "\n",
    "- triplet은 얼굴 이미지의 인코딩을 학습하는데 효과적인 손실함수입니다.\n",
    "\n",
    "- 인코딩으로 만들어진 벡터는 verification과 recognition에 사용됩니다. 두 인코딩된 벡터의 거리는 두 이미지가 같은 사람인지 결정하게 도와줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "- Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf) \n",
    "- The pretrained model we use is inspired by Victor Sy Wang's implementation and was loaded using his code: https://github.com/iwantooxxoox/Keras-OpenFace.\n",
    "- Our implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet \n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "IaknP",
   "launcher_item_id": "5UMr4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
