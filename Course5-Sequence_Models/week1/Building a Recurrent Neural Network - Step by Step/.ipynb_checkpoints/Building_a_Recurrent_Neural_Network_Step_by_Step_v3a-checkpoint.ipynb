{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your Recurrent Neural Network - Step by Step\n",
    "\n",
    "이번 과정에서는 RNN의 핵심 구성요소들을 numpy를 사용해 구현해볼겁니다.\n",
    "\n",
    "RNN은 기억을 할 수 있어 자연어 처리 또는 순서대로 작동하는 업무(시계열)에 효율적입니다. \n",
    "\n",
    "입력 값으로 여러개 단어로된 문장(x)을 받는다면 한번에 한 단어씩($x^{\\langle t \\rangle}$) 받고 그 단어에 대한 정보를 저장합니다. 저장된 정보는 다음 단어가 들어올때 문맥을 이해하는데 참고로 사용됩니다\n",
    "\n",
    "이러한 방식은 unidirectional RNN으로 표현하고 현재 입력된 단어가 과거에 사용된 단어들에게만 영향을 받는다는 의미입니다.  \n",
    "\n",
    "좀 더 문맥을 잘 이해하는 방법으로는 bidirectional RNN을 사용합니다. bidirectional은 과거와 미래 정보를 다 반영해 현재 단어를 이해하도록 합니다.\n",
    "\n",
    "\n",
    "**표기법**:\n",
    "- 위첨자 $[l]$ 는 $i$번째 layer를 의미합니다.\n",
    "\n",
    "- 위첨자 $(i)$는 $i$번째 데이터 샘플을 의미합니다.\n",
    "\n",
    "- 위첨자 $\\langle t \\rangle$는 입력데이터의 $i$번째 데이터를 의미합니다.\n",
    "    \n",
    "- 아래첨자 $i$는 벡터의 $i$번째 값을 의미합니다.\n",
    "\n",
    "예시:  \n",
    "- $a^{(2)[3]<4>}_5$ denotes the activation of the 2nd training example (2), 3rd layer [3], 4th time step <4>, and 5th entry in the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 과정에 필요한 패키지를 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rnn_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Forward propagation for the basic Recurrent Neural Network\n",
    "\n",
    "기본적인 RNN 구조는 다음과 같습니다. (보여지는 예시는 $T_x = T_y$ 조건에 해당합니다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/RNN.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure 1**: Basic RNN model </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions of input $x$\n",
    "\n",
    "#### Input with $n_x$ number of units\n",
    "* 입력데이터의 한개 샘플$x^{(i)}$은 1차원짜리 벡터입니다.\n",
    "* 만약 5000개 단어로 설명하는 문장을 사용한다면 한개 샘플$x^{(i)}$은 one-hot 인코딩으로 인해 5000개 값으로 이루어진 벡터로 만들어집니다. 따라서 $x^{(i)}$의 구조는 (5000, )입니다.\n",
    "\n",
    "* 학습 데이터 샘플의 1개 unit을 $n_x$로 표현하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batches of size $m$\n",
    "* 학습데이터 샘플 20개를 batch size 라고 생각해봅시다.\n",
    "\n",
    "* 벡터화로 이점을 얻기 위해 $x^{(i)}$ 20개를 묶어 2차원 행렬로 표현하겠습니다.\n",
    "\n",
    "* 예를 들어 텐서의 구조가 (5000,20)되도록 말입니다.\n",
    "\n",
    "* 그리고 사용할 학습데이터 샘플의 갯수를 의미하는 $m$를 사용하겠습니다.\n",
    "\n",
    "* 최종적으로 mini-batch는 $(n_x,m)$구조를 갖습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time steps of size $T_{x}$\n",
    "* RNN에는 순서가 있습니다. 순서를 의미하는 $t$를 사용합니다.\n",
    "\n",
    "* 이번 과정에서 한개 학습데이터 샘플$x^{(i)}$을 순서대로 입력되는 모습을 확인할겁니다. 만약 들어가는 순서가 10개라면 $T_{x} = 10$라고 표현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D Tensor of shape $(n_{x},m,T_{x})$\n",
    "* RNN 에 입력값 $x$ 는 3차원 구조를 갖게 되며  $(n_x,m,T_x)$ 구조가 됩니다.\n",
    "\n",
    "#### Taking a 2D slice for each time step: $x^{\\langle t \\rangle}$\n",
    "* 각 순서에서 학습데이터의 mini-batch 단위를 사용할겁니다.\n",
    "* 각 순서$t$에서 $(n_x,m)$구조의 데이터를 사용합니다.\n",
    "* 위 2차원 자료 구조는 $x^{\\langle t \\rangle}$로 사용되고 변수명은 `xt`로 사용하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of hidden state $a$\n",
    "\n",
    "* 각 순서에서 입력된 값이 활성함수를 거치게 된 값$a^{\\langle t \\rangle}$입니다.\n",
    "\n",
    "### Dimensions of hidden state $a$\n",
    "\n",
    "* 입력 값 $x$와 거의 비슷합니다. 만약 한개짜리 학습데이터 샘플이라면 $a$는 1차원 벡터이고 길이는 $n_{a}$로 표현합니다.\n",
    "\n",
    "* 만약 mini-batch 사이즈$m$을 사용한다면 구조는 $(n_{a},m)$가 됩니다.\n",
    "\n",
    "* 순서에 관한 차원을 추가한다면 구조가 $(n_{a}, m, T_x)$와 같이 됩니다.\n",
    "\n",
    "* 각 단계에 입력되는 값은 $a^{\\langle t \\rangle}$로 표현합니다.\n",
    "\n",
    "* 코드에서 변수명을 `a_prev`와 `a_next`을 사용합니다. (이전 순서의 값과 다음에 사용될 값)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions of prediction $\\hat{y}$\n",
    "* $\\hat{y}$ 또한 비슷한 구조입니다. $(n_{y}, m, T_{y})$.  \n",
    "    * $n_{y}$: 벡터 길이를 의미합니다.\n",
    "    * $m$: mini-batch 사이즈입니다.\n",
    "    * $T_{y}$: 몇번째 순서의 예측값인지를 의미합니다.\n",
    "* 한개 순서는 $t$이고 해당 위치에서 나오는 $\\hat{y}^{\\langle t \\rangle}$는 $(n_{y}, m)$구조를 갖습니다.\n",
    "\n",
    "* 코드에서 사용할 변수 명:\n",
    "    - `y_pred`: $\\hat{y}$ \n",
    "    - `yt_pred`: $\\hat{y}^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**:\n",
    "1. RNN 에서 한 순서에 대한 값을 계산해야합니다.\n",
    "\n",
    "2. 순서대로 입력값을 계산해야합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - RNN cell\n",
    "\n",
    "RNN은 한개 cell을 반복하는 것처럼 볼수 있습니다.\n",
    "\n",
    "한 순서에 대해 계산을 일단 해봅시다.\n",
    "\n",
    "다음 코드는 한 순서를 계산하는 것을 묘사합니다\n",
    "\n",
    "\n",
    "<img src=\"images/rnn_step_forward_figure2_v3a.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center>  \n",
    "    **Figure 2**: 일반적인 RNN cell에서는 현재 단계의 $x^{\\langle t \\rangle}$를 받고 이전 단계에서 나온 정보 $a^{\\langle t - 1\\rangle}$를 받습니다.  <br>\n",
    "    그리고 출력 값으로 다음 cell에 사용될 $a^{\\langle t \\rangle}$와 예측값인 $\\hat{y}^{\\langle t \\rangle}$가 나오게 됩니다. </center></caption>\n",
    "\n",
    "#### rnn cell versus rnn_cell_forward\n",
    "* RNN cell에서 출력값 $a^{\\langle t \\rangle}$이 나옵니다.  \n",
    "    * rnn cell은 위 사진에서 안쪽 실선으로 표시된 부분을 의미합니다.\n",
    "* `rnn_cell_forward` 부분에서는 예측값인 $\\hat{y}^{\\langle t \\rangle}$가 출력됩니다.\n",
    "    * rnn_cell_forward은 위 사진에서 겉에 점선으로 표시된 부분입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: 위 그림과 같이 RNN-cell을 구현해봅시다.\n",
    "\n",
    "**Instructions**:\n",
    "1. 활성함수로 tanh를 사용합니다. $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
    "\n",
    "2. 활성함수를 거친 값으로 예측값 $\\hat{y}^{\\langle t \\rangle}$를 계산합니다. $\\hat{y}^{\\langle t \\rangle} =softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$.\n",
    "\n",
    "3. $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$ 를 `cache`에 저장합니다.\n",
    "4. $a^{\\langle t \\rangle}$ , $\\hat{y}^{\\langle t \\rangle}$과 `cache`를 반환합니다\n",
    "\n",
    "#### Additional Hints\n",
    "* [numpy.tanh](https://www.google.com/search?q=numpy+tanh&rlz=1C5CHFA_enUS854US855&oq=numpy+tanh&aqs=chrome..69i57j0l5.1340j0j7&sourceid=chrome&ie=UTF-8)\n",
    "* 'rnn_utils.py'에 `softmax`를 구현해놨습니다.\n",
    "* 행렬간의 곱을 위해 [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)를 참고하세요~\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: rnn_cell_forward\n",
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN-cell as described in Figure (2)\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba  -- Bias, numpy array of shape (n_a, 1)\n",
    "                        by  -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    Returns:\n",
    "    a_next  -- next hidden state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache   -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba  = parameters[\"ba\"]\n",
    "    by  = parameters[\"by\"]\n",
    "\n",
    "    # compute next activation state using the formula given above\n",
    "    a_next  = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)\n",
    "    # compute output of the current cell using the formula given above\n",
    "    yt_pred = softmax( np.dot(Wya, a_next) + by )   \n",
    "\n",
    "    # store values you need for backward propagation in cache\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] = \n",
      " [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
      " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
      "a_next.shape = \n",
      " (5, 10)\n",
      "yt_pred[1] =\n",
      " [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212\n",
      "  0.36920224  0.9966312   0.9982559   0.17746526]\n",
      "yt_pred.shape = \n",
      " (2, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt_tmp = np.random.randn(3,10)     # batch size = 10\n",
    "a_prev_tmp = np.random.randn(5,10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
    "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
    "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
    "parameters_tmp['ba']  = np.random.randn(5,1)\n",
    "parameters_tmp['by']  = np.random.randn(2,1)\n",
    "\n",
    "a_next_tmp, yt_pred_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\n",
    "print(\"a_next[4] = \\n\", a_next_tmp[4])\n",
    "print(\"a_next.shape = \\n\", a_next_tmp.shape)\n",
    "print(\"yt_pred[1] =\\n\", yt_pred_tmp[1])\n",
    "print(\"yt_pred.shape = \\n\", yt_pred_tmp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "```Python\n",
    "a_next[4] = \n",
    " [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
    " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
    "a_next.shape = \n",
    " (5, 10)\n",
    "yt_pred[1] =\n",
    " [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212\n",
    "  0.36920224  0.9966312   0.9982559   0.17746526]\n",
    "yt_pred.shape = \n",
    " (2, 10)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - RNN forward pass \n",
    "\n",
    "- RNN 은 방근 만든 RNN cell을 반복해 구축됩니다.\n",
    "    - 만약 입력데이터의 순서가 10단계라면 RNN cell을 10번 사용해야합니다.\n",
    "- 각 cell(각 순서)에는 두 가지 입력값을 받습니다:\n",
    "    - $a^{\\langle t-1 \\rangle}$: 이전 cell에서 얻은 활성함수 값\n",
    "    - $x^{\\langle t \\rangle}$: 현재 cell에서 얻은 입력데이터\n",
    "- 각 cell(각 순서)에서 두 가지 출력값을 반환합니다:\n",
    "    - ($a^{\\langle t \\rangle}$) 활성함수 값\n",
    "    - ($y^{\\langle t \\rangle}$) 예측 값\n",
    "- 매개변수 기울기와 편향 $(W_{aa}, b_{a}, W_{ax}, b_{x})$ 은 매 순서에 같은 값을 사용합니다. \n",
    "    - 매개변수 기울기와 편향을 'parameters'에 저장되어 유지됩니다.\n",
    "\n",
    "<img src=\"images/rnn_forward_sequence_figure3_v3a.png\" style=\"width:800px;height:180px;\">\n",
    "<caption><center> **Figure 3**: Basic RNN. The input sequence $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  is carried over $T_x$ time steps. The network outputs $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. </center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: 위 그림과 같이 forward propagation RNN을 구현해봅시다.\n",
    "\n",
    "**Instructions**:\n",
    "* 새로 만들어질 $a$를 저장하기 위한 3차원 영행렬을 만듭니다. 구조는 $(n_{a}, m, T_{x})$를 갖습니다. RNN cell을 반복할때마다 값이 채워질겁니다.\n",
    "\n",
    "* 새로 만들어질 $\\hat{y}$를 저장하기 위한 3차원 영행렬을 만듭니다. 구조는 $(n_{y}, m, T_{x})$를 갖습니다. RNN cell을 반복할때마다 값이 채워질겁니다.\n",
    "    - 이번 과정에서는 $T_{y} = T_{x}$이라는 조건을 가정합니다. (출력값 갯수가 입력값 갯수가 다른 예제도 있습니다.!!)\n",
    "\n",
    "* 각 순서에서:\n",
    "    - 각 순서($T_{x}$)에서 $x^{\\langle t \\rangle}$를 받습니다.\n",
    "        - $x^{\\langle t \\rangle}$의 구조는 $(n_{x}, m)$입니다.\n",
    "        - 참고로 $x$는 $(n_{x}, m, T_{x})$구조입니다.\n",
    "    - `rnn_cell_forward`를 사용해 $a^{\\langle t \\rangle}$(변수명: `a_next`)와 예측값 $\\hat{y}^{\\langle t \\rangle}$ 그리고 cache를 업데이트 합니다.\n",
    "        - $a^{\\langle t \\rangle}$의 구조는 $(n_{a}, m)$입니다.\n",
    "    - 각 순서에서 생긴 $a^{\\langle t \\rangle}$와 $\\hat{y}^{\\langle t \\rangle}$를 3차원으로 합친 뒤 $a$와 $\\hat{y}_{pred}$로 저장합니다.\n",
    "        - $a$의 구조는 $(n_{a}, m, T_{x})$입니다\n",
    "        - $\\hat{y}^{\\langle t \\rangle}$의 구조는 $(n_{y}, m)$입니다.\n",
    "        - $\\hat{y}$의 구조는 $(n_{y}, m, T_x)$입니다.\n",
    "    - 그리고 업데이트 되는 cache도 누적시켜 저장합니다.\n",
    "* 최종 적으로 $a$와 $\\hat{y}$ 그리고 caches를 반환합니다.\n",
    "\n",
    "#### Additional Hints\n",
    "- [np.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html)\n",
    "- 만약 3차원 행렬에서 부분적으로 2차원을 선택할때는 다음과 같이 작성합니다. `var_name[:,:,i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: rnn_forward\n",
    "def rnn_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network described in Figure (3).\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \"\"\"\n",
    "    # Initialize \"caches\" which will contain the list of all caches\n",
    "    caches = []\n",
    "    \n",
    "    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a    = parameters[\"Wya\"].shape\n",
    "\n",
    "    # initialize \"a\" and \"y_pred\" with zeros (≈2 lines)\n",
    "    a      = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next (≈1 line)\n",
    "    a_next = np.copy(a0)\n",
    "    \n",
    "    # loop over all time-steps of the input 'x' (1 line)\n",
    "    for t in range(T_x):\n",
    "        # Update next hidden state, compute the prediction, get the cache (≈2 lines)\n",
    "        xt                     = x[:,:,t]\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(xt, a_next, parameters) # rnn_cell_forward에 입력값이 a_next 라고 표시되어있어도 의미로는 a_prev로 사용됩니다\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t]               = a_next\n",
    "        # Save the value of the prediction in y (≈1 line)\n",
    "        y_pred[:,:,t]          = yt_pred\n",
    "        # Append \"cache\" to \"caches\" (≈1 line)\n",
    "        caches.append( cache )\n",
    "\n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "    \n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][1] = \n",
      " [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
      "a.shape = \n",
      " (5, 10, 4)\n",
      "y_pred[1][3] =\n",
      " [ 0.79560373  0.86224861  0.11118257  0.81515947]\n",
      "y_pred.shape = \n",
      " (2, 10, 4)\n",
      "caches[1][1][3] =\n",
      " [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
      "len(caches) = \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x_tmp = np.random.randn(3,10,4)\n",
    "a0_tmp = np.random.randn(5,10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
    "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
    "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
    "parameters_tmp['ba'] = np.random.randn(5,1)\n",
    "parameters_tmp['by'] = np.random.randn(2,1)\n",
    "\n",
    "a_tmp, y_pred_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)\n",
    "print(\"a[4][1] = \\n\", a_tmp[4][1])\n",
    "print(\"a.shape = \\n\", a_tmp.shape)\n",
    "print(\"y_pred[1][3] =\\n\", y_pred_tmp[1][3])\n",
    "print(\"y_pred.shape = \\n\", y_pred_tmp.shape)\n",
    "print(\"caches[1][1][3] =\\n\", caches_tmp[1][1][3])\n",
    "print(\"len(caches) = \\n\", len(caches_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```Python\n",
    "a[4][1] = \n",
    " [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
    "a.shape = \n",
    " (5, 10, 4)\n",
    "y_pred[1][3] =\n",
    " [ 0.79560373  0.86224861  0.11118257  0.81515947]\n",
    "y_pred.shape = \n",
    " (2, 10, 4)\n",
    "caches[1][1][3] =\n",
    " [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
    "len(caches) = \n",
    " 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Situations when this RNN will perform better:\n",
    "- 방금 구현한 RNN은 어느정도 괜찮은 성과를 낼수 있습니다. 하지만 vanishing gradient 문제가 있습니다.\n",
    "\n",
    "- 각 순서의 $\\hat{y}^{\\langle t \\rangle}$는 \"local\" context를 사용해 구하면 더 정확합니다.  \n",
    "\n",
    "- \"Local\" context refers to information that is close to the prediction's time step $t$.\n",
    "\n",
    "- More formally, local context refers to inputs $x^{\\langle t' \\rangle}$ and predictions $\\hat{y}^{\\langle t \\rangle}$ where $t'$ is close to $t$.\n",
    "\n",
    "다음 단계로 vanishing gradients를 해결할 수 있는 LSTM모델을 만들어보겠습니다. LSTM은 이전 단어를 더 오랬동안 기억할 수 있는 장점이 또 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Long Short-Term Memory (LSTM) network\n",
    "\n",
    "<img src=\"images/LSTM_figure4_v3a.png\" style=\"width:500;height:400px;\">\n",
    "<caption><center> **Figure 4**: LSTM-cell. This tracks and updates a \"cell state\" or memory variable $c^{\\langle t \\rangle}$ at every time-step, which can be different from $a^{\\langle t \\rangle}$. </center></caption>\n",
    "\n",
    "위 RNN 예제에서 했던 방법과 같이 1 cell 만 구현하도록하겠습니다.\n",
    "\n",
    "이후 1 cell용으로 만든 기능을 반복문을 통해 여러번의 time-step을 거치도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of gates and states\n",
    "\n",
    "#### - Forget gate $\\mathbf{\\Gamma}_{f}$\n",
    "\n",
    "* 우리가 문장의 단어들을 읽고있다고 가정해봅시다. 그리고 LSTM을 사용해 문맥 구조를 이해하려고 한다고 해봅시다. 예를 들어 주어가 단수일 때 \"puppy\"를 쓰고 복수 일때 \"puppies\"를 사용하는 것을 목표로 생각합시다.\n",
    "\n",
    "* 만약 주어가 단수에서 복수로 바뀐다면 기억하고 있던 정보는 사라져야합니다. 그래서 Forget gate를 사용해 예전 기억을 제거합니다.\n",
    "\n",
    "* \"forget gate\"는 0과 1사이 값을 갖는 텐서입니다.\n",
    "    * 만약 forget gate에 값이 0에 가깝다면 LSTM은 forget gate에 대응되는 $c^{\\langle t \\rangle}$를 잊게됩니다.\n",
    "    * 만약 forget gate에 값이 1에 가깝다면 LSTM은 forget gate에 대응되는 $c^{\\langle t \\rangle}$를 유지합니다.\n",
    "\n",
    "\n",
    "##### Equation\n",
    "\n",
    "$$\\mathbf{\\Gamma}_f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)\\tag{1} $$\n",
    "\n",
    "##### Explanation of the equation:\n",
    "\n",
    "* $\\mathbf{W_{f}}$ 는 기울기로 forget gate의 행동을 조절합니다.\n",
    "\n",
    "* 이전 단계의 $a^{\\langle t-1 \\rangle}$ 와 현재 단계의 $x^{\\langle t \\rangle}$를 묶어 $\\mathbf{W_{f}}$와 연산합니다.\n",
    "\n",
    "* sigmoid를 사용해 $\\mathbf{\\Gamma}_f^{\\langle t \\rangle}$의 값 범위가 0~1사이로 만듭니다.\n",
    "\n",
    "* forget gate  $\\mathbf{\\Gamma}_f^{\\langle t \\rangle}$ 는 이전 cell state $c^{\\langle t-1 \\rangle}$와 같은 구조입니다. \n",
    "\n",
    "* 현재 forget gate와 이전 층의 cell state가 같은 구조이기 때문에 서로 매핑되는 부분끼리 곱할수 있습니다.\n",
    "\n",
    "* $\\mathbf{\\Gamma}_f^{\\langle t \\rangle} * \\mathbf{c}^{\\langle t-1 \\rangle}$ 은 이전 cell state에 마스크를 씌워 필터링하는 작업과 비슷합니다.\n",
    "\n",
    "* 자연스럽게 $\\mathbf{\\Gamma}_f^{\\langle t \\rangle}$에서 값이 0에 가깝거나 0이라면 연산 결과 또한 0 또는 0에 가깝겠죠?\n",
    "    * 그럼 이전 층의 $\\mathbf{c}^{\\langle t-1 \\rangle}$와 매핑되는 부분은 기억에서 지워지는 영향을 받게 됩니다\n",
    "    \n",
    "* 비슷하게 $\\mathbf{\\Gamma}_f^{\\langle t \\rangle}$에서 값이 1에 가깝거나 1이라면 연산 결과가 이전값과 유사하게 유지되겠죠?\n",
    "    * $\\mathbf{c}^{\\langle t-1 \\rangle}$와 매핑되는 부분은 다음 단계 까지 유지되어 보내집니다.\n",
    "    \n",
    "##### Variable names in the code\n",
    "* `Wf`: forget gate weight $\\mathbf{W}_{f}$\n",
    "* `Wb`: forget gate bias $\\mathbf{W}_{b}$\n",
    "* `ft`: forget gate $\\Gamma_f^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candidate value $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$\n",
    "* candidate value 는 현재 시점의 cell state$\\mathbf{c}^{\\langle t \\rangle}$에 저장되어야할 정보들을 갖고있습니다.\n",
    "\n",
    "* candidate value는 update gate에 의해 출력값으로 전달됩니다.\n",
    "\n",
    "* candidate value 값의 범위는 -! ~ 1 사이값입니다.\n",
    "\n",
    "* \"~\" 표시는 현재 시점에서 만들어지는 $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ 와 cell state $\\mathbf{c}^{\\langle t \\rangle}$를 구분하기 위함입니다..\n",
    "\n",
    "##### Equation\n",
    "$$\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{a}^{\\langle t - 1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right) \\tag{3}$$\n",
    "\n",
    "##### Explanation of the equation\n",
    "* 활성함수 'tanh'는 출력값이 -1 ~ 1 사이값을 갖습니다.\n",
    "\n",
    "\n",
    "##### Variable names in the code\n",
    "* `cct`: candidate value $\\mathbf{\\tilde{c}}^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Update gate $\\mathbf{\\Gamma}_{i}$\n",
    "\n",
    "* update gate를 사용해 새롭게 추가될 정보인 $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$를 cell state에 어느정도 영향을 줄지 정합니다.\n",
    "\n",
    "* update gate의 값들은 0~1 사이 값을 갖습니다.\n",
    "    * update gate 단일 값이 1에 가깝다면 매핑되는 $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ 값이 $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$와 $a^{\\langle t \\rangle}$에 영향이 갑니다.\n",
    "    * update gate 단일 값이 0에 가깝다면 매핑되는 $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ 값이 $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$와 $a^{\\langle t \\rangle}$에 새로운 영향이 가지 않습니다.\n",
    "\n",
    "##### Equation\n",
    "\n",
    "$$\\mathbf{\\Gamma}_i^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_i[a^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_i)\\tag{2} $$ \n",
    "\n",
    "##### Explanation of the equation\n",
    "\n",
    "* forget gate와 유사한 용도입니다. $\\mathbf{\\Gamma}_i^{\\langle t \\rangle}$의 값은 시그모이드로 인해 0~1 사이 값을 갖습니다.\n",
    "* update gate도 매핑되는 $\\tilde{c}^{\\langle t \\rangle}$의 값들과 연산되어 $\\mathbf{c}^{\\langle t \\rangle}$를 구합니다.\n",
    "\n",
    "##### Variable names in code (Please note that they're different than the equations)\n",
    "코드에서는 논문에 사용된 변수명을 따라 쓰겠습니다.\n",
    "* `Wi` is the update gate weight $\\mathbf{W}_i$ (not \"Wu\") \n",
    "* `bi` is the update gate bias $\\mathbf{b}_i$ (not \"bu\")\n",
    "* `it` is the forget gate $\\mathbf{\\Gamma}_i^{\\langle t \\rangle}$ (not \"ut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Cell state $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "\n",
    "* The cell state is the \"memory\" that gets passed onto future time steps.\n",
    "* The new cell state $\\mathbf{c}^{\\langle t \\rangle}$ is a combination of the previous cell state and the candidate value.\n",
    "\n",
    "##### Equation\n",
    "\n",
    "$$ \\mathbf{c}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_f^{\\langle t \\rangle}* \\mathbf{c}^{\\langle t-1 \\rangle} + \\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} *\\mathbf{\\tilde{c}}^{\\langle t \\rangle} \\tag{4} $$\n",
    "\n",
    "##### Explanation of equation\n",
    "* The previous cell state $\\mathbf{c}^{\\langle t-1 \\rangle}$ is adjusted (weighted) by the forget gate $\\mathbf{\\Gamma}_{f}^{\\langle t \\rangle}$\n",
    "* and the candidate value $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$, adjusted (weighted) by the update gate $\\mathbf{\\Gamma}_{i}^{\\langle t \\rangle}$\n",
    "\n",
    "##### Variable names and shapes in the code\n",
    "* `c`: cell state, including all time steps, $\\mathbf{c}$ shape $(n_{a}, m, T)$\n",
    "* `c_next`: new (next) cell state, $\\mathbf{c}^{\\langle t \\rangle}$ shape $(n_{a}, m)$\n",
    "* `c_prev`: previous cell state, $\\mathbf{c}^{\\langle t-1 \\rangle}$, shape $(n_{a}, m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Output gate $\\mathbf{\\Gamma}_{o}$\n",
    "\n",
    "* The output gate decides what gets sent as the prediction (output) of the time step.\n",
    "* The output gate is like the other gates. It contains values that range from 0 to 1.\n",
    "\n",
    "##### Equation\n",
    "\n",
    "$$ \\mathbf{\\Gamma}_o^{\\langle t \\rangle}=  \\sigma(\\mathbf{W}_o[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})\\tag{5}$$ \n",
    "\n",
    "##### Explanation of the equation\n",
    "* The output gate is determined by the previous hidden state $\\mathbf{a}^{\\langle t-1 \\rangle}$ and the current input $\\mathbf{x}^{\\langle t \\rangle}$\n",
    "* The sigmoid makes the gate range from 0 to 1.\n",
    "\n",
    "\n",
    "##### Variable names in the code\n",
    "* `Wo`: output gate weight, $\\mathbf{W_o}$\n",
    "* `bo`: output gate bias, $\\mathbf{b_o}$\n",
    "* `ot`: output gate, $\\mathbf{\\Gamma}_{o}^{\\langle t \\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Hidden state $\\mathbf{a}^{\\langle t \\rangle}$\n",
    "\n",
    "* The hidden state gets passed to the LSTM cell's next time step.\n",
    "* It is used to determine the three gates ($\\mathbf{\\Gamma}_{f}, \\mathbf{\\Gamma}_{u}, \\mathbf{\\Gamma}_{o}$) of the next time step.\n",
    "* The hidden state is also used for the prediction $y^{\\langle t \\rangle}$.\n",
    "\n",
    "##### Equation\n",
    "\n",
    "$$ \\mathbf{a}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_o^{\\langle t \\rangle} * \\tanh(\\mathbf{c}^{\\langle t \\rangle})\\tag{6} $$\n",
    "\n",
    "##### Explanation of equation\n",
    "* The hidden state $\\mathbf{a}^{\\langle t \\rangle}$ is determined by the cell state $\\mathbf{c}^{\\langle t \\rangle}$ in combination with the output gate $\\mathbf{\\Gamma}_{o}$.\n",
    "* The cell state state is passed through the \"tanh\" function to rescale values between -1 and +1.\n",
    "* The output gate acts like a \"mask\" that either preserves the values of $\\tanh(\\mathbf{c}^{\\langle t \\rangle})$ or keeps those values from being included in the hidden state $\\mathbf{a}^{\\langle t \\rangle}$\n",
    "\n",
    "##### Variable names  and shapes in the code\n",
    "* `a`: hidden state, including time steps.  $\\mathbf{a}$ has shape $(n_{a}, m, T_{x})$\n",
    "* 'a_prev`: hidden state from previous time step. $\\mathbf{a}^{\\langle t-1 \\rangle}$ has shape $(n_{a}, m)$\n",
    "* `a_next`: hidden state for next time step.  $\\mathbf{a}^{\\langle t \\rangle}$ has shape $(n_{a}, m)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Prediction $\\mathbf{y}^{\\langle t \\rangle}_{pred}$\n",
    "* The prediction in this use case is a classification, so we'll use a softmax.\n",
    "\n",
    "The equation is:\n",
    "$$\\mathbf{y}^{\\langle t \\rangle}_{pred} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{a}^{\\langle t \\rangle} + \\mathbf{b}_{y})$$\n",
    "\n",
    "##### Variable names and shapes in the code\n",
    "* `y_pred`: prediction, including all time steps. $\\mathbf{y}_{pred}$ has shape $(n_{y}, m, T_{x})$.  Note that $(T_{y} = T_{x})$ for this example.\n",
    "* `yt_pred`: prediction for the current time step $t$. $\\mathbf{y}^{\\langle t \\rangle}_{pred}$ has shape $(n_{y}, m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - LSTM cell\n",
    "\n",
    "**Exercise**: Implement the LSTM cell described in the Figure (4).\n",
    "\n",
    "**Instructions**:\n",
    "1. Concatenate the hidden state $a^{\\langle t-1 \\rangle}$ and input $x^{\\langle t \\rangle}$ into a single matrix:  \n",
    "\n",
    "$$concat = \\begin{bmatrix} a^{\\langle t-1 \\rangle} \\\\ x^{\\langle t \\rangle} \\end{bmatrix}$$  \n",
    "\n",
    "2. Compute all the formulas 1 through 6 for the gates, hidden state, and cell state.\n",
    "3. Compute the prediction $y^{\\langle t \\rangle}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Hints\n",
    "* You can use [numpy.concatenate](https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html).  Check which value to use for the `axis` parameter.\n",
    "* The functions `sigmoid()` and `softmax` are imported from `rnn_utils.py`.\n",
    "* [numpy.tanh](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html)\n",
    "* Use [np.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) for matrix multiplication.\n",
    "* Notice that the variable names `Wi`, `bi` refer to the weights and biases of the **update** gate.  There are no variables named \"Wu\" or \"bu\" in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: lstm_cell_forward\n",
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implement a single forward step of the LSTM-cell as described in Figure (4)\n",
    "\n",
    "    Arguments:\n",
    "    xt     -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        bc --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)                 \n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    c_next -- next memory state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "    \n",
    "    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n",
    "          c stands for the cell state (memory)\n",
    "    \"\"\"\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wf = parameters[\"Wf\"] # forget gate weight\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"] # update gate weight (notice the variable name)\n",
    "    bi = parameters[\"bi\"] # (notice the variable name)\n",
    "    Wc = parameters[\"Wc\"] # candidate value weight\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"] # output gate weight\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"] # prediction weight\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # Retrieve dimensions from shapes of xt and Wy\n",
    "    n_x, m   = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    # Concatenate a_prev and xt (≈1 line)\n",
    "#     print('a_prev', a_prev.shape)\n",
    "#     print('xt',     xt.shape)\n",
    "    concat = np.concatenate((a_prev, xt), axis=0)\n",
    "\n",
    "    # Compute values for ft (forget gate), it (update gate),\n",
    "    # cct (candidate value), c_next (cell state), \n",
    "    # ot (output gate), a_next (hidden state) (≈6 lines)\n",
    "    ft     = sigmoid( np.dot(Wf, concat) + bf )    # forget gate\n",
    "    it     = sigmoid( np.dot(Wi, concat) + bi )    # update gate\n",
    "    cct    = np.tanh( np.dot(Wc, concat) + bc )    # candidate value\n",
    "    c_next = ft * c_prev + it * cct                # cell state\n",
    "    ot     = sigmoid( np.dot(Wo, concat) + bo )    # output gate\n",
    "    a_next = ot * np.tanh(c_next)                  # hidden state\n",
    "    \n",
    "    # Compute prediction of the LSTM cell (≈1 line)\n",
    "    yt_pred = softmax( np.dot(Wy, a_next) + by )\n",
    "\n",
    "    # store values needed for backward propagation in cache\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] = \n",
      " [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
      "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
      "a_next.shape =  (5, 10)\n",
      "c_next[2] = \n",
      " [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
      "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
      "c_next.shape =  (5, 10)\n",
      "yt[1] = [ 0.79913913  0.15986619  0.22412122  0.15606108  0.97057211  0.31146381\n",
      "  0.00943007  0.12666353  0.39380172  0.07828381]\n",
      "yt.shape =  (2, 10)\n",
      "cache[1][3] =\n",
      " [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
      "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
      "len(cache) =  10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt_tmp = np.random.randn(3,10)\n",
    "a_prev_tmp = np.random.randn(5,10)\n",
    "c_prev_tmp = np.random.randn(5,10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bf'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wi'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bi'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bo'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bc'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wy'] = np.random.randn(2,5)\n",
    "parameters_tmp['by'] = np.random.randn(2,1)\n",
    "\n",
    "a_next_tmp, c_next_tmp, yt_tmp, cache_tmp = lstm_cell_forward(xt_tmp, a_prev_tmp, c_prev_tmp, parameters_tmp)\n",
    "print(\"a_next[4] = \\n\", a_next_tmp[4])\n",
    "print(\"a_next.shape = \", c_next_tmp.shape)\n",
    "print(\"c_next[2] = \\n\", c_next_tmp[2])\n",
    "print(\"c_next.shape = \", c_next_tmp.shape)\n",
    "print(\"yt[1] =\", yt_tmp[1])\n",
    "print(\"yt.shape = \", yt_tmp.shape)\n",
    "print(\"cache[1][3] =\\n\", cache_tmp[1][3])\n",
    "print(\"len(cache) = \", len(cache_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```Python\n",
    "a_next[4] = \n",
    " [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
    "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
    "a_next.shape =  (5, 10)\n",
    "c_next[2] = \n",
    " [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
    "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
    "c_next.shape =  (5, 10)\n",
    "yt[1] = [ 0.79913913  0.15986619  0.22412122  0.15606108  0.97057211  0.31146381\n",
    "  0.00943007  0.12666353  0.39380172  0.07828381]\n",
    "yt.shape =  (2, 10)\n",
    "cache[1][3] =\n",
    " [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
    "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
    "len(cache) =  10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Forward pass for LSTM\n",
    "\n",
    "Now that you have implemented one step of an LSTM, you can now iterate this over this using a for-loop to process a sequence of $T_x$ inputs. \n",
    "\n",
    "<img src=\"images/LSTM_rnn.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure 5**: LSTM over multiple time-steps. </center></caption>\n",
    "\n",
    "**Exercise:** Implement `lstm_forward()` to run an LSTM over $T_x$ time-steps. \n",
    "\n",
    "**Instructions**\n",
    "* Get the dimensions $n_x, n_a, n_y, m, T_x$ from the shape of the variables: `x` and `parameters`.\n",
    "* Initialize the 3D tensors $a$, $c$ and $y$.\n",
    "    - $a$: hidden state, shape $(n_{a}, m, T_{x})$\n",
    "    - $c$: cell state, shape $(n_{a}, m, T_{x})$\n",
    "    - $y$: prediction, shape $(n_{y}, m, T_{x})$ (Note that $T_{y} = T_{x}$ in this example).\n",
    "    - **Note** Setting one variable equal to the other is a \"copy by reference\".  In other words, don't do `c = a', otherwise both these variables point to the same underlying variable.\n",
    "* Initialize the 2D tensor $a^{\\langle t \\rangle}$ \n",
    "    - $a^{\\langle t \\rangle}$ stores the hidden state for time step $t$.  The variable name is `a_next`.\n",
    "    - $a^{\\langle 0 \\rangle}$, the initial hidden state at time step 0, is passed in when calling the function. The variable name is `a0`.\n",
    "    - $a^{\\langle t \\rangle}$ and $a^{\\langle 0 \\rangle}$ represent a single time step, so they both have the shape  $(n_{a}, m)$ \n",
    "    - Initialize $a^{\\langle t \\rangle}$ by setting it to the initial hidden state ($a^{\\langle 0 \\rangle}$) that is passed into the function.\n",
    "* Initialize $c^{\\langle t \\rangle}$ with zeros. \n",
    "    - The variable name is `c_next`. \n",
    "    - $c^{\\langle t \\rangle}$ represents a single time step, so its shape is $(n_{a}, m)$\n",
    "    - **Note**: create `c_next` as its own variable with its own location in memory.  Do not initialize it as a slice of the 3D tensor $c$.  In other words, **don't** do `c_next = c[:,:,0]`.\n",
    "* For each time step, do the following:\n",
    "    - From the 3D tensor $x$, get a 2D slice $x^{\\langle t \\rangle}$ at time step $t$.\n",
    "    - Call the `lstm_cell_forward` function that you defined previously, to get the hidden state, cell state, prediction, and cache.\n",
    "    - Store the hidden state, cell state and prediction (the 2D tensors) inside the 3D tensors.\n",
    "    - Also append the cache to the list of caches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: lstm_forward\n",
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (4).\n",
    "\n",
    "    Arguments:\n",
    "    x  -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "                        \n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    c -- The value of the cell state, numpy array of shape (n_a, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
    "    \"\"\"\n",
    "    # Initialize \"caches\", which will track the list of all the caches\n",
    "    caches = []\n",
    "\n",
    "    Wy = parameters['Wy'] # saving parameters['Wy'] in a local variable in case students use Wy instead of parameters['Wy']\n",
    "    # Retrieve dimensions from shapes of x and parameters['Wy'] (≈2 lines)\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a    = Wy.shape\n",
    "    \n",
    "    # initialize \"a\", \"c\" and \"y\" with zeros (≈3 lines)\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    c = np.zeros((n_a, m, T_x))\n",
    "    y = np.zeros((n_y, m, T_x))\n",
    "    \n",
    "    # Initialize a_next and c_next (≈2 lines)\n",
    "    a_next = np.copy(a0)\n",
    "    c_next = np.zeros((n_a, m))\n",
    "    \n",
    "    # loop over all time-steps\n",
    "    for t in range(T_x):\n",
    "        # Get the 2D slice 'xt' from the 3D input 'x' at time step 't'\n",
    "        xt       = x[:,:,t]\n",
    "        # Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)\n",
    "        a_next, c_next, yt, cache = lstm_cell_forward(xt, a_next, c_next, parameters)\n",
    "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
    "        a[:,:,t] = a_next\n",
    "        # Save the value of the next cell state (≈1 line)\n",
    "        c[:,:,t]= c_next\n",
    "        # Save the value of the prediction in y (≈1 line)\n",
    "        y[:,:,t] = yt\n",
    "        # Append the cache into caches (≈1 line)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "\n",
    "    return a, y, c, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][3][6] =  0.172117767533\n",
      "a.shape =  (5, 10, 7)\n",
      "y[1][4][3] = 0.95087346185\n",
      "y.shape =  (2, 10, 7)\n",
      "caches[1][1][1] =\n",
      " [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
      "  0.41005165]\n",
      "c[1][2][1] -0.855544916718\n",
      "len(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x_tmp = np.random.randn(3,10,7)\n",
    "a0_tmp = np.random.randn(5,10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bf'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wi'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bi']= np.random.randn(5,1)\n",
    "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bo'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bc'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wy'] = np.random.randn(2,5)\n",
    "parameters_tmp['by'] = np.random.randn(2,1)\n",
    "\n",
    "a_tmp, y_tmp, c_tmp, caches_tmp = lstm_forward(x_tmp, a0_tmp, parameters_tmp)\n",
    "print(\"a[4][3][6] = \", a_tmp[4][3][6])\n",
    "print(\"a.shape = \", a_tmp.shape)\n",
    "print(\"y[1][4][3] =\", y_tmp[1][4][3])\n",
    "print(\"y.shape = \", y_tmp.shape)\n",
    "print(\"caches[1][1][1] =\\n\", caches_tmp[1][1][1])\n",
    "print(\"c[1][2][1]\", c_tmp[1][2][1])\n",
    "print(\"len(caches) = \", len(caches_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```Python\n",
    "a[4][3][6] =  0.172117767533\n",
    "a.shape =  (5, 10, 7)\n",
    "y[1][4][3] = 0.95087346185\n",
    "y.shape =  (2, 10, 7)\n",
    "caches[1][1][1] =\n",
    " [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
    "  0.41005165]\n",
    "c[1][2][1] -0.855544916718\n",
    "len(caches) =  2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have now implemented the forward passes for the basic RNN and the LSTM. When using a deep learning framework, implementing the forward pass is sufficient to build systems that achieve great performance. \n",
    "\n",
    "The rest of this notebook is optional, and will not be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Backpropagation in recurrent neural networks (OPTIONAL / UNGRADED)\n",
    "\n",
    "In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers do not need to bother with the details of the backward pass. If however you are an expert in calculus and want to see the details of backprop in RNNs, you can work through this optional portion of the notebook. \n",
    "\n",
    "When in an earlier course you implemented a simple (fully connected) neural network, you used backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in recurrent neural networks you can calculate the derivatives with respect to the cost in order to update the parameters. The backprop equations are quite complicated and we did not derive them in lecture. However, we will briefly present them below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Basic RNN  backward pass\n",
    "\n",
    "We will start by computing the backward pass for the basic RNN-cell.\n",
    "\n",
    "<img src=\"images/rnn_cell_backprop.png\" style=\"width:500;height:300px;\"> <br>\n",
    "<caption><center> **Figure 6**: RNN-cell's backward pass. Just like in a fully-connected neural network, the derivative of the cost function $J$ backpropagates through the RNN by following the chain-rule from calculus. The chain-rule is also used to calculate $(\\frac{\\partial J}{\\partial W_{ax}},\\frac{\\partial J}{\\partial W_{aa}},\\frac{\\partial J}{\\partial b})$ to update the parameters $(W_{ax}, W_{aa}, b_a)$. </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving the one step backward functions: \n",
    "\n",
    "To compute the `rnn_cell_backward` you need to compute the following equations. It is a good exercise to derive them by hand. \n",
    "\n",
    "The derivative of $\\tanh$ is $1-\\tanh(x)^2$. You can find the complete proof [here](https://www.wyzant.com/resources/lessons/math/calculus/derivative_proofs/tanx). Note that: $ \\text{sech}(x)^2 = 1 - \\tanh(x)^2$\n",
    "\n",
    "Similarly for $\\frac{ \\partial a^{\\langle t \\rangle} } {\\partial W_{ax}}, \\frac{ \\partial a^{\\langle t \\rangle} } {\\partial W_{aa}},  \\frac{ \\partial a^{\\langle t \\rangle} } {\\partial b}$, the derivative of  $\\tanh(u)$ is $(1-\\tanh(u)^2)du$. \n",
    "\n",
    "The final two equations also follow the same rule and are derived using the $\\tanh$ derivative. Note that the arrangement is done in a way to get the same dimensions to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_cell_backward(da_next, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward pass for the RNN-cell (single time-step).\n",
    "\n",
    "    Arguments:\n",
    "    da_next -- Gradient of loss with respect to next hidden state\n",
    "    cache   -- python dictionary containing useful values (output of rnn_cell_forward())\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradients of input data, of shape (n_x, m)\n",
    "                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dba -- Gradients of bias vector, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    # Retrieve values from cache\n",
    "    (a_next, a_prev, xt, parameters) = cache\n",
    "    \n",
    "    # Retrieve values from parameters\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba  = parameters[\"ba\"]\n",
    "    by  = parameters[\"by\"]\n",
    "\n",
    "    # compute the gradient of tanh with respect to a_next (≈1 line)\n",
    "    dtanh = (1 - a_next**2) * da_next    \n",
    "\n",
    "    # compute the gradient of the loss with respect to Wax (≈2 lines)\n",
    "    dxt  = np.dot( Wax.T, dtanh )\n",
    "    dWax = np.dot( dtanh, xt.T )\n",
    "\n",
    "\n",
    "    # compute the gradient with respect to Waa (≈2 lines)\n",
    "    da_prev = np.dot( Waa.T, dtanh )\n",
    "    dWaa    = np.dot( dtanh, a_prev.T  )\n",
    "\n",
    "    # compute the gradient with respect to b (≈1 line)\n",
    "    dba     = np.sum( dtanh, axis=-1, keepdims=True )\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dxt\"][1][2] = -1.3872130506\n",
      "gradients[\"dxt\"].shape = (3, 10)\n",
      "gradients[\"da_prev\"][2][3] = -0.152399493774\n",
      "gradients[\"da_prev\"].shape = (5, 10)\n",
      "gradients[\"dWax\"][3][1] = 0.410772824935\n",
      "gradients[\"dWax\"].shape = (5, 3)\n",
      "gradients[\"dWaa\"][1][2] = 1.15034506685\n",
      "gradients[\"dWaa\"].shape = (5, 5)\n",
      "gradients[\"dba\"][4] = [ 0.20023491]\n",
      "gradients[\"dba\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt_tmp = np.random.randn(3,10)\n",
    "a_prev_tmp = np.random.randn(5,10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
    "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
    "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
    "parameters_tmp['ba'] = np.random.randn(5,1)\n",
    "parameters_tmp['by'] = np.random.randn(2,1)\n",
    "\n",
    "a_next_tmp, yt_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\n",
    "\n",
    "da_next_tmp = np.random.randn(5,10)\n",
    "gradients_tmp = rnn_cell_backward(da_next_tmp, cache_tmp)\n",
    "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients_tmp[\"dxt\"][1][2])\n",
    "print(\"gradients[\\\"dxt\\\"].shape =\", gradients_tmp[\"dxt\"].shape)\n",
    "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients_tmp[\"da_prev\"][2][3])\n",
    "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients_tmp[\"da_prev\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients_tmp[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients_tmp[\"dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients_tmp[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients_tmp[\"dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"][4] =\", gradients_tmp[\"dba\"][4])\n",
    "print(\"gradients[\\\"dba\\\"].shape =\", gradients_tmp[\"dba\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dxt\"][1][2]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           -0.460564103059\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dxt\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (3, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da_prev\"][2][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           0.0842968653807\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da_prev\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWax\"][3][1]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           0.393081873922\n",
    "        </td>\n",
    "    </tr>\n",
    "            <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWax\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 3)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWaa\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           -0.28483955787\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWaa\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 5)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dba\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.80517166]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dba\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass through the RNN\n",
    "\n",
    "Computing the gradients of the cost with respect to $a^{\\langle t \\rangle}$ at every time-step $t$ is useful because it is what helps the gradient backpropagate to the previous RNN-cell. To do so, you need to iterate through all the time steps starting at the end, and at each step, you increment the overall $db_a$, $dW_{aa}$, $dW_{ax}$ and you store $dx$.\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "Implement the `rnn_backward` function. Initialize the return variables with zeros first and then loop through all the time steps while calling the `rnn_cell_backward` at each time timestep, update the other variables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_backward(da, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for a RNN over an entire sequence of input data.\n",
    "\n",
    "    Arguments:\n",
    "    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)\n",
    "    caches -- tuple containing information from the forward pass (rnn_forward)\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)\n",
    "                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)\n",
    "                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)\n",
    "                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy array of shape (n_a, n_a)\n",
    "                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    # Retrieve values from the first cache (t=1) of caches (≈2 lines)\n",
    "    (caches, x)              = caches\n",
    "    (a1, a0, x1, parameters) = caches[0]\n",
    "    \n",
    "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m      = x1.shape\n",
    "    \n",
    "    # initialize the gradients with the right sizes (≈6 lines)\n",
    "    dx   = np.zeros((n_x, m, T_x))\n",
    "    dWax = np.zeros((parameters['Wax'].shape))\n",
    "    dWaa = np.zeros((parameters['Waa'].shape))\n",
    "    dba  = np.zeros((parameters['ba'].shape))\n",
    "    da0  = np.zeros(a0.shape)\n",
    "    da_prevt = np.zeros((n_a, m))\n",
    "    \n",
    "    # Loop through all the time steps\n",
    "    for t in reversed(range(T_x)):\n",
    "        # Compute gradients at time step t. Choose wisely the \"da_next\" and the \"cache\" to use in the backward propagation step. (≈1 line)\n",
    "        gradients = rnn_cell_backward( da[:, :, t] + da_prevt, caches[t])\n",
    "        # Retrieve derivatives from gradients (≈ 1 line)\n",
    "        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[\"dxt\"], gradients[\"da_prev\"], gradients[\"dWax\"], gradients[\"dWaa\"], gradients[\"dba\"]\n",
    "        # Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)\n",
    "        dx[:, :, t] = dxt\n",
    "        dWax += dWaxt\n",
    "        dWaa += dWaat\n",
    "        dba  += dbat\n",
    "        \n",
    "    # Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) \n",
    "    da0 = da_prevt\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dx\"][1][2] = [-2.07101689 -0.59255627  0.02466855  0.01483317]\n",
      "gradients[\"dx\"].shape = (3, 10, 4)\n",
      "gradients[\"da0\"][2][3] = -0.314942375127\n",
      "gradients[\"da0\"].shape = (5, 10)\n",
      "gradients[\"dWax\"][3][1] = 11.2641044965\n",
      "gradients[\"dWax\"].shape = (5, 3)\n",
      "gradients[\"dWaa\"][1][2] = 2.30333312658\n",
      "gradients[\"dWaa\"].shape = (5, 5)\n",
      "gradients[\"dba\"][4] = [-0.74747722]\n",
      "gradients[\"dba\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x_tmp = np.random.randn(3,10,4)\n",
    "a0_tmp = np.random.randn(5,10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
    "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
    "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
    "parameters_tmp['ba'] = np.random.randn(5,1)\n",
    "parameters_tmp['by'] = np.random.randn(2,1)\n",
    "\n",
    "a_tmp, y_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)\n",
    "da_tmp = np.random.randn(5, 10, 4)\n",
    "gradients_tmp = rnn_backward(da_tmp, caches_tmp)\n",
    "\n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients_tmp[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients_tmp[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients_tmp[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients_tmp[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients_tmp[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients_tmp[\"dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients_tmp[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients_tmp[\"dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"][4] =\", gradients_tmp[\"dba\"][4])\n",
    "print(\"gradients[\\\"dba\\\"].shape =\", gradients_tmp[\"dba\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dx\"][1][2]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           [-2.07101689 -0.59255627  0.02466855  0.01483317]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dx\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (3, 10, 4)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da0\"][2][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           -0.314942375127\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da0\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "         <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWax\"][3][1]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           11.2641044965\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWax\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 3)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWaa\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           2.30333312658\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWaa\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 5)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dba\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.74747722]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dba\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - LSTM backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 One Step backward\n",
    "\n",
    "The LSTM backward pass is slightly more complicated than the forward one. We have provided you with all the equations for the LSTM backward pass below. (If you enjoy calculus exercises feel free to try deriving these from scratch yourself.) \n",
    "\n",
    "### 3.2.2 gate derivatives\n",
    "\n",
    "$$d \\Gamma_o^{\\langle t \\rangle} = da_{next}*\\tanh(c_{next}) * \\Gamma_o^{\\langle t \\rangle}*(1-\\Gamma_o^{\\langle t \\rangle})\\tag{7}$$\n",
    "\n",
    "$$d\\tilde c^{\\langle t \\rangle} = dc_{next}*\\Gamma_u^{\\langle t \\rangle}+ \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * i_t * da_{next} * \\tilde c^{\\langle t \\rangle} * (1-\\tanh(\\tilde c)^2) \\tag{8}$$\n",
    "\n",
    "$$d\\Gamma_u^{\\langle t \\rangle} = dc_{next}*\\tilde c^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * \\tilde c^{\\langle t \\rangle} * da_{next}*\\Gamma_u^{\\langle t \\rangle}*(1-\\Gamma_u^{\\langle t \\rangle})\\tag{9}$$\n",
    "\n",
    "$$d\\Gamma_f^{\\langle t \\rangle} = dc_{next}*\\tilde c_{prev} + \\Gamma_o^{\\langle t \\rangle} (1-\\tanh(c_{next})^2) * c_{prev} * da_{next}*\\Gamma_f^{\\langle t \\rangle}*(1-\\Gamma_f^{\\langle t \\rangle})\\tag{10}$$\n",
    "\n",
    "### 3.2.3 parameter derivatives \n",
    "\n",
    "$$ dW_f = d\\Gamma_f^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{11} $$\n",
    "$$ dW_u = d\\Gamma_u^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{12} $$\n",
    "$$ dW_c = d\\tilde c^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{13} $$\n",
    "$$ dW_o = d\\Gamma_o^{\\langle t \\rangle} * \\begin{pmatrix} a_{prev} \\\\ x_t\\end{pmatrix}^T \\tag{14}$$\n",
    "\n",
    "To calculate $db_f, db_u, db_c, db_o$ you just need to sum across the horizontal (axis= 1) axis on $d\\Gamma_f^{\\langle t \\rangle}, d\\Gamma_u^{\\langle t \\rangle}, d\\tilde c^{\\langle t \\rangle}, d\\Gamma_o^{\\langle t \\rangle}$ respectively. Note that you should have the `keep_dims = True` option.\n",
    "\n",
    "Finally, you will compute the derivative with respect to the previous hidden state, previous memory state, and input.\n",
    "\n",
    "$$ da_{prev} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c^{\\langle t \\rangle} + W_o^T * d\\Gamma_o^{\\langle t \\rangle} \\tag{15}$$\n",
    "Here, the weights for equations 13 are the first n_a, (i.e. $W_f = W_f[:n_a,:]$ etc...)\n",
    "\n",
    "$$ dc_{prev} = dc_{next}\\Gamma_f^{\\langle t \\rangle} + \\Gamma_o^{\\langle t \\rangle} * (1- \\tanh(c_{next})^2)*\\Gamma_f^{\\langle t \\rangle}*da_{next} \\tag{16}$$\n",
    "$$ dx^{\\langle t \\rangle} = W_f^T*d\\Gamma_f^{\\langle t \\rangle} + W_u^T * d\\Gamma_u^{\\langle t \\rangle}+ W_c^T * d\\tilde c_t + W_o^T * d\\Gamma_o^{\\langle t \\rangle}\\tag{17} $$\n",
    "where the weights for equation 15 are from n_a to the end, (i.e. $W_f = W_f[n_a:,:]$ etc...)\n",
    "\n",
    "**Exercise:** Implement `lstm_cell_backward` by implementing equations $7-17$ below. Good luck! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell_backward(da_next, dc_next, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for the LSTM-cell (single time-step).\n",
    "\n",
    "    Arguments:\n",
    "    da_next -- Gradients of next hidden state, of shape (n_a, m)\n",
    "    dc_next -- Gradients of next cell state, of shape (n_a, m)\n",
    "    cache -- cache storing information from the forward pass\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dxt -- Gradient of input data at time-step t, of shape (n_x, m)\n",
    "                        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "                        dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)\n",
    "                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
    "                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
    "                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
    "                        dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    # Retrieve information from \"cache\"\n",
    "    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from xt's and a_next's shape (≈2 lines)\n",
    "    n_x, m = None\n",
    "    n_a, m = None\n",
    "    \n",
    "    # Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) (≈4 lines)\n",
    "    dot = None\n",
    "    dcct = None\n",
    "    dit = None\n",
    "    dft = None\n",
    "    \n",
    "    # Code equations (7) to (10) (≈4 lines)\n",
    "    dit = None\n",
    "    dft = None\n",
    "    dot = None\n",
    "    dcct = None\n",
    "\n",
    "    # Compute parameters related derivatives. Use equations (11)-(14) (≈8 lines)\n",
    "    dWf = None\n",
    "    dWi = None\n",
    "    dWc = None\n",
    "    dWo = None\n",
    "    dbf = None\n",
    "    dbi = None\n",
    "    dbc = None\n",
    "    dbo = None\n",
    "\n",
    "    # Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). (≈3 lines)\n",
    "    da_prev = None\n",
    "    dc_prev = None\n",
    "    dxt = None\n",
    "    \n",
    "    # Save gradients in dictionary\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "xt_tmp = np.random.randn(3,10)\n",
    "a_prev_tmp = np.random.randn(5,10)\n",
    "c_prev_tmp = np.random.randn(5,10)\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bf'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wi'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bi'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bo'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bc'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wy'] = np.random.randn(2,5)\n",
    "parameters_tmp['by'] = np.random.randn(2,1)\n",
    "\n",
    "a_next_tmp, c_next_tmp, yt_tmp, cache_tmp = lstm_cell_forward(xt_tmp, a_prev_tmp, c_prev_tmp, parameters_tmp)\n",
    "\n",
    "da_next_tmp = np.random.randn(5,10)\n",
    "dc_next_tmp = np.random.randn(5,10)\n",
    "gradients_tmp = lstm_cell_backward(da_next_tmp, dc_next_tmp, cache_tmp)\n",
    "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients_tmp[\"dxt\"][1][2])\n",
    "print(\"gradients[\\\"dxt\\\"].shape =\", gradients_tmp[\"dxt\"].shape)\n",
    "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients_tmp[\"da_prev\"][2][3])\n",
    "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients_tmp[\"da_prev\"].shape)\n",
    "print(\"gradients[\\\"dc_prev\\\"][2][3] =\", gradients_tmp[\"dc_prev\"][2][3])\n",
    "print(\"gradients[\\\"dc_prev\\\"].shape =\", gradients_tmp[\"dc_prev\"].shape)\n",
    "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients_tmp[\"dWf\"][3][1])\n",
    "print(\"gradients[\\\"dWf\\\"].shape =\", gradients_tmp[\"dWf\"].shape)\n",
    "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients_tmp[\"dWi\"][1][2])\n",
    "print(\"gradients[\\\"dWi\\\"].shape =\", gradients_tmp[\"dWi\"].shape)\n",
    "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients_tmp[\"dWc\"][3][1])\n",
    "print(\"gradients[\\\"dWc\\\"].shape =\", gradients_tmp[\"dWc\"].shape)\n",
    "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients_tmp[\"dWo\"][1][2])\n",
    "print(\"gradients[\\\"dWo\\\"].shape =\", gradients_tmp[\"dWo\"].shape)\n",
    "print(\"gradients[\\\"dbf\\\"][4] =\", gradients_tmp[\"dbf\"][4])\n",
    "print(\"gradients[\\\"dbf\\\"].shape =\", gradients_tmp[\"dbf\"].shape)\n",
    "print(\"gradients[\\\"dbi\\\"][4] =\", gradients_tmp[\"dbi\"][4])\n",
    "print(\"gradients[\\\"dbi\\\"].shape =\", gradients_tmp[\"dbi\"].shape)\n",
    "print(\"gradients[\\\"dbc\\\"][4] =\", gradients_tmp[\"dbc\"][4])\n",
    "print(\"gradients[\\\"dbc\\\"].shape =\", gradients_tmp[\"dbc\"].shape)\n",
    "print(\"gradients[\\\"dbo\\\"][4] =\", gradients_tmp[\"dbo\"][4])\n",
    "print(\"gradients[\\\"dbo\\\"].shape =\", gradients_tmp[\"dbo\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dxt\"][1][2]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           3.23055911511\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dxt\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (3, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da_prev\"][2][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           -0.0639621419711\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da_prev\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "         <tr>\n",
    "        <td>\n",
    "            **gradients[\"dc_prev\"][2][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           0.797522038797\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dc_prev\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWf\"][3][1]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           -0.147954838164\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWf\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWi\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           1.05749805523\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWi\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWc\"][3][1]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           2.30456216369\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWc\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWo\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           0.331311595289\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWo\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbf\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.18864637]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbf\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbi\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.40142491]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbi\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbc\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.25587763]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbc\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbo\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.13893342]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbo\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Backward pass through the LSTM RNN\n",
    "\n",
    "This part is very similar to the `rnn_backward` function you implemented above. You will first create variables of the same dimension as your return variables. You will then iterate over all the time steps starting from the end and call the one step function you implemented for LSTM at each iteration. You will then update the parameters by summing them individually. Finally return a dictionary with the new gradients. \n",
    "\n",
    "**Instructions**: Implement the `lstm_backward` function. Create a for loop starting from $T_x$ and going backward. For each step call `lstm_cell_backward` and update the your old gradients by adding the new gradients to them. Note that `dxt` is not updated but is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).\n",
    "\n",
    "    Arguments:\n",
    "    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)\n",
    "    caches -- cache storing information from the forward pass (lstm_forward)\n",
    "\n",
    "    Returns:\n",
    "    gradients -- python dictionary containing:\n",
    "                        dx -- Gradient of inputs, of shape (n_x, m, T_x)\n",
    "                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)\n",
    "                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)\n",
    "                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)\n",
    "                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)\n",
    "                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve values from the first cache (t=1) of caches.\n",
    "    (caches, x) = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
    "    \n",
    "    # Retrieve dimensions from da's and x1's shapes (≈2 lines)\n",
    "    n_a, m, T_x = None\n",
    "    n_x, m = None\n",
    "    \n",
    "    # initialize the gradients with the right sizes (≈12 lines)\n",
    "    dx = None\n",
    "    da0 = None\n",
    "    da_prevt = None\n",
    "    dc_prevt = None\n",
    "    dWf = None\n",
    "    dWi = None\n",
    "    dWc = None\n",
    "    dWo = None\n",
    "    dbf = None\n",
    "    dbi = None\n",
    "    dbc = None\n",
    "    dbo = None\n",
    "    \n",
    "    # loop back over the whole sequence\n",
    "    for t in reversed(range(None)):\n",
    "        # Compute all gradients using lstm_cell_backward\n",
    "        gradients = None\n",
    "        # Store or add the gradient to the parameters' previous step's gradient\n",
    "        dx[:,:,t] = None\n",
    "        dWf = None\n",
    "        dWi = None\n",
    "        dWc = None\n",
    "        dWo = None\n",
    "        dbf = None\n",
    "        dbi = None\n",
    "        dbc = None\n",
    "        dbo = None\n",
    "    # Set the first activation's gradient to the backpropagated gradient da_prev.\n",
    "    da0 = None\n",
    "\n",
    "    # Store the gradients in a python dictionary\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x_tmp = np.random.randn(3,10,7)\n",
    "a0_tmp = np.random.randn(5,10)\n",
    "\n",
    "parameters_tmp = {}\n",
    "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bf'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wi'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bi'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bo'] = np.random.randn(5,1)\n",
    "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
    "parameters_tmp['bc'] = np.random.randn(5,1)\n",
    "\n",
    "a_tmp, y_tmp, c_tmp, caches_tmp = lstm_forward(x_tmp, a0_tmp, parameters_tmp)\n",
    "\n",
    "da_tmp = np.random.randn(5, 10, 4)\n",
    "gradients_tmp = lstm_backward(da_tmp, caches_tmp)\n",
    "\n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients_tmp[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients_tmp[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients_tmp[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients_tmp[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients_tmp[\"dWf\"][3][1])\n",
    "print(\"gradients[\\\"dWf\\\"].shape =\", gradients_tmp[\"dWf\"].shape)\n",
    "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients_tmp[\"dWi\"][1][2])\n",
    "print(\"gradients[\\\"dWi\\\"].shape =\", gradients_tmp[\"dWi\"].shape)\n",
    "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients_tmp[\"dWc\"][3][1])\n",
    "print(\"gradients[\\\"dWc\\\"].shape =\", gradients_tmp[\"dWc\"].shape)\n",
    "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients_tmp[\"dWo\"][1][2])\n",
    "print(\"gradients[\\\"dWo\\\"].shape =\", gradients_tmp[\"dWo\"].shape)\n",
    "print(\"gradients[\\\"dbf\\\"][4] =\", gradients_tmp[\"dbf\"][4])\n",
    "print(\"gradients[\\\"dbf\\\"].shape =\", gradients_tmp[\"dbf\"].shape)\n",
    "print(\"gradients[\\\"dbi\\\"][4] =\", gradients_tmp[\"dbi\"][4])\n",
    "print(\"gradients[\\\"dbi\\\"].shape =\", gradients_tmp[\"dbi\"].shape)\n",
    "print(\"gradients[\\\"dbc\\\"][4] =\", gradients_tmp[\"dbc\"][4])\n",
    "print(\"gradients[\\\"dbc\\\"].shape =\", gradients_tmp[\"dbc\"].shape)\n",
    "print(\"gradients[\\\"dbo\\\"][4] =\", gradients_tmp[\"dbo\"][4])\n",
    "print(\"gradients[\\\"dbo\\\"].shape =\", gradients_tmp[\"dbo\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dx\"][1][2]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.00173313  0.08287442 -0.30545663 -0.43281115]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dx\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (3, 10, 4)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da0\"][2][3]** =\n",
    "        </td>\n",
    "        <td>\n",
    "           -0.095911501954\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"da0\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 10)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWf\"][3][1]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           -0.0698198561274\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWf\"].shape** =\n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWi\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           0.102371820249\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWi\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWc\"][3][1]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           -0.0624983794927\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWc\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWo\"][1][2]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           0.0484389131444\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dWo\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbf\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.0565788]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbf\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbi\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.06997391]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbi\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbc\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [-0.27441821]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbc\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbo\"][4]** = \n",
    "        </td>\n",
    "        <td>\n",
    "           [ 0.16532821]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **gradients[\"dbo\"].shape** = \n",
    "        </td>\n",
    "        <td>\n",
    "           (5, 1)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "xxuVc",
   "launcher_item_id": "X20PE"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
